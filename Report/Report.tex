
\documentclass[11pt]{article}

\usepackage{graphicx}
\usepackage{framed}
\usepackage{listings}
\usepackage{titlesec}
\usepackage{tikz}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{MnSymbol}
\usepackage{stmaryrd}
\usepackage{multirow}
\usepackage[margin=1.6in]{geometry}
\usepackage{rotating}

\newcommand{\sectionbreak}{\clearpage}


\lstset{frame=tb,
  language=Java,
  aboveskip=5mm,
  belowskip=5mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  breaklines=true,
  breakatwhitespace=true
  tabsize=3
}


\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}

\newenvironment{proof}[1][Proof]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{definition}[1][Definition]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{example}[1][Example]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{remark}[1][Remark]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}



\newcommand{\qed}{\nobreak \ifvmode \relax \else
      \ifdim\lastskip<1.5em \hskip-\lastskip
      \hskip1.5em plus0em minus0.5em \fi \nobreak
      \vrule height0.75em width0.5em depth0.25em\fi}
\newcommand{\pre}{\mathop{\mathrm{pre}}}
\newcommand{\myparagraph}[1]{\paragraph{#1}\mbox{}\\}



\begin{document}

\tableofcontents
\clearpage
\begin{abstract}

\end{abstract}

\section{Introduction}

\subsection{The Problem}

Verifying computer systems is essential. They have become such an important part of our lives that unforeseen software bugs and malfunction can lead to disastrous situations. [finir le paragraphe avec des exemples] 

Our dependency on computer systems is increasing, but so is their \textit{complexity} which, more often than not, demands a large amount of skilled human maintenance. Financially, this has motivated the need for systems to be more independent, and consequently, a large number of the critical systems that we rely on are now \textit{autonomous}: they are able to ``make decisions" without human intervention, based on a variety of factors such as their current state or location. 

The verification of autonomous systems has been a very active research area in the past few decades due to their rising importance, and a range of verification techniques have been developed based on formal methods for modelling systems. These include automated theorem proving, TODO, or \textit{model checking}, the latter being the focus of this project.
\\\\

 Model checking was introduced in 1981 by E. M. Clarke and E. A. Emerson \cite{original_model_checking}, and independently by J. P. Queille and J. Sifakis \cite{symbolic_model_checking}, and is found to have major advantages \cite{birth_of_model_checking}: no correctness proofs are required (it is an automatic process), it finds counterexamples (i.e. bugs), and it allows for many different properties to be checked, by means of temporal logics. 
 
Unfortunately, an important disadvantage of model checking, known as the \textit{state explosion problem}, is the difficulty in verifying larger systems efficiently. In the past 20 years, several techniques attempting to solve the state explosion problem have been introduced, such as predicate abstraction, bounded model checking, symmetry reduction, and (perhaps more notably, at least for this project) symbolic model checking with \textit{ordered binary decision diagrams} (OBDDs).

The idea of symbolic model checking, introduced in 1992 by K. L. McMillan \cite{symbolic}, is to encode states and transitions of the system as \textit{Boolean functions}, and represent these symbolically using efficient and compact data structures such as OBDDs. Symbolic model checking with OBDDs has allowed for much larger systems to be verified, and has therefore seen a rise in the popularity of model checking as a verification technique.
\\\\

Although OBDD-based techniques have significantly improved the efficiency of model checking, it is 



\subsection{The Idea}

In this project we propose an alternative to OBDDs for symbolic model checking: another representation of Boolean functions

\subsection{The Deliverables}


\begin{itemize}
\item Autonomous systems, what they are, why they are useful, and why they are vulnerable but we need to make sure they are reliable.
\item Verifying autonomous systems: what it entails, how we go about it (methods) and what are the challenges
\item The state explosion problem
\item Our idea and contributions
\item structure of the report
\end{itemize}

\section{Background}

\subsection{Logics for Multi-Agent Systems}

\subsubsection{Multi-Agent Systems}

Autonomous multi-agent systems are computer systems which are made up of several intelligent ``agents" acting within an ``environment". 
Intuitively, an \textit{agent} is: 
\begin{itemize}
\item Capable of \textit{autonomous} action 
\item Capable of \textit{social} interaction with its peers
\item Acting to \textit{meet} their design objectives 
\end{itemize}
 
Suppose we have a multi-agent systems consisting of $n$ agents and an environment $e$.
\begin{definition} 
An agent $i$ in the system consists of: 
\begin{itemize}
\item A set $L_i$ of local states representing the different configurations of the agent,
\item A set $Act_i$ of local actions that the agent can take,
\item A protocol function $P_i : L_i \rightarrow 2^{Act_i} $ expressing the decision making of the agent.
\end{itemize} 
\end{definition}

We can define the environment $e$ as a similar structure $(L_e, Act_e, P_e)$ where $P_e$ represents the functioning conditions of the environment. 

\subsubsection{Interpreted Systems}
\label{IS}
We present a formal structure to represent multi-agent systems. Consider a multi-agent system $\Sigma$ consisting of $n$ agents $1, ..., n$ and an environment $e$.
\begin{definition}
An \textit{Interpreted System} $IS$ for $\Sigma$ is a tuple $(G, \tau, I, \sim_1, ..., \sim_n, \pi)$, where
\begin{itemize}
\item $G \subseteq L_1 \times ... \times L_n \times L_e$ is the set of global states that $\Sigma$ can reach. A global state $g \in G$ is essentially a picture of the system at a given point in time, and the local state of agent $i$ in $g$ is denoted $l_i(g)$.
\item $I \subseteq G$ is a set of intial states for the system
\item $\tau : G \times Act \rightarrow G $ where $Act = Act_1 \times ... \times Act_n \times Act_e$ is a deterministic transition function (we can define $\tau : G \times Act \rightarrow 2^G$ to model a non-deterministic system)
\item $\sim_1, ..., \sim_n \subseteq G \times G$  are binary relations defined by $$g \sim_i g' \Leftrightarrow l_i(g) = l_i(g') \quad \forall g, g' \in G, \forall i = 1, ..., n$$
i.e iff agent $i$ is in the same state in both $g$ and $g'$. 
\item $\pi : PV \rightarrow G$ is a valuation function for the set of atoms $PV$, i.e for each atom $p \in PV$, $\pi(p)$ is the set of global states where $p$ is true
\end{itemize}
\end{definition}
We also need a formal definition for the ``execution'' of a system. A \emph{run}, as defined below, represents one possible execution of a MAS. 
\begin{definition} 
A \emph{run} of an interpreted system $IS = (G, \tau, I, \sim_1, ..., \sim_n, \pi)$ is a sequence $r = g_0, g_1, ...$ where $g_0 \in I$ and such that for all $i \geq 0$, $\exists a \in Act$ such that $\tau(g_i, a) = g_{i+1}$.
\end{definition}

Interpreted systems as defined above are used as semantic structures for a particular family of logics, presented in the next section.

\subsubsection{Linear Temporal Logic} 
\label{LTL}

In order to verify properties of multi-agent systems, we first need to find a logic allowing us to describe these properties accurately. 

A good candidate is the Linear Temporal Logic (LTL), a modal temporal logic in which one can write formulas about the future of \emph{paths}. Here we use paths to represent infinite runs of an interpreted system. 

\begin{definition} 
The syntax of LTL formulas is given by the following BNF: 
$$\varphi := p \mid \lnot\varphi \mid \varphi \land \varphi \mid X\varphi \mid G\varphi \mid \varphi U\varphi$$
\end{definition}

The intuitive meanings of $X\varphi, G\varphi$ and $\varphi U\psi$ are respectively 
\begin{itemize} 
\item $\varphi$ holds at the ne\textbf{X}t time instant
\item $\varphi$ holds forever (\textbf{G}lobally)
\item $\varphi$ holds \textbf{U}ntil $\psi$ holds  
\end{itemize}

We define the unary operator $F$ to be the dual of $G$, i.e $F\varphi := \lnot G\lnot\varphi$ for any LTL formula $\varphi$. $F\varphi$ represents the idea that $\varphi$ will hold at some point in the \textbf{F}uture.   

\myparagraph{Semantics} 

A model for LTL is a Kripke model $M = (W, R, \pi)$ such that the relation $R$ is serial, i.e $\forall u \in W,  \exists v \in W$ such that $(u, v) \in R$.
The worlds in $W$ are called the \textit{states} of the model. 

\begin{definition} 
A \textit{path} in an LTL model $M = (W, R, \pi)$ is an infinite sequence of states $\rho = s_0, s_1, ...$ such that $(s_i, s_{i+1}) \in R$ for any $i \geq 0$.
We denote $\rho^i$ the suffix of $\rho$ starting at $i$ (note that $\rho^i$ is itself a path since $\rho$ is infinite).
\end{definition}

It is easy to see how such a model can be used to represent a computer system, and how an execution of this system can be written as a path.

 Our objective is to be able to verify that a system $S$ has property $P$, so if we encode $P$ as an LTL formula $\varphi_P$ and $S$ as a model $M_S$, then we need to be able to check whether $\varphi_P$ is \textit{valid} in $M$ (or at least true in a set of initial states). This technique is called \textit{model checking}, and we do this by using the following definition for the semantics of LTL:
\label{model_checking}

\begin{definition}
Given LTL formulae $\varphi$ and $\psi$, a model $M$ and a state $s_0 \in W$, we say that
\begin{eqnarray*}
(M, s_0) \models p &\Leftrightarrow& s_0 \in \pi(p) \\  
(M, s_0) \models \lnot \varphi &\Leftrightarrow& (M, s_0) \nmodels \lnot \varphi\\
(M, s_0) \models \varphi \land \psi &\Leftrightarrow& (M, s_0) \models \varphi \mbox{ and  } (M, s_0) \models \psi \\
(M, s_0) \models X\varphi &\Leftrightarrow& (M, s_1) \models \varphi \mbox{  for all states } s_1 \mbox{ such that } R(s_0, s_1)\\
(M, s_0) \models G\varphi &\Leftrightarrow& \mbox{for all paths } \rho = s_0, s_1, s_2, ... , \mbox{ we have } (M, s_i) \models \varphi \quad \forall i \geq 0 \\
(M, s_0) \models \varphi U\psi &\Leftrightarrow& \mbox{for all paths } \rho = s_0, s_1, s_2, ..., \exists j \geq 0 \mbox{ such that }  (M, s_j) \models \psi \\ && \mbox{ and }  (M, s_k) \models \varphi \quad \forall 0 \leq k < j
\end{eqnarray*}

\end{definition}
The expressive power of LTL is limited to quantification over \textit{all} possible paths. For example: 

\begin{itemize}
\item $FG($deadlocked$)$ 

In every possible execution, the system will be permanently deadlocked.


\item $GF($crash$)$

Whatever happens, the system will crash infinitely often.
\end{itemize}
Hence some properties cannot be expressed in LTL, as in certain applications we might want to quantify explicitely over paths. The Computation Tree Logic (CTL) can express this. 

\subsubsection{Computation Tree Logic} 

\begin{definition} 
The syntax of CTL formulae is defined as follows: 
$$ \varphi := p \mid \lnot \varphi \mid \varphi \land \varphi \mid EX\varphi \mid EG\varphi \mid E(\varphi U \varphi)$$
\end{definition}

Intuitively, $ EX\varphi,  EG\varphi,$ and $ E(\varphi U \psi)$ represent the fact that there exists a possible path starting from the current state such that, respectively, $\varphi$ is true at the next state, $\varphi$ holds forever in the future, and $\varphi$ holds until $\psi$ becomes true.

The dual operator $AX\varphi := \lnot EX \lnot\varphi $ can be used to represent the fact that in all possible paths from the current state, $\varphi$ is true at the next state.
Connectives $AG\varphi, AF\varphi,$ and $A(\varphi U\psi)$ can be defined in the same way. 

We also use models (as defined in \ref{LTL}) for the semantics of CTL, as follows:

\begin{definition} 
Given CTL formulas $\varphi$ and $\psi$, a model $M = (W, R, \pi)$ and a state $s_0 \in W$, the satisfaction of formulas at $s_0$ in $M$ is defined inductively as follows: 
\begin{eqnarray*}
(M, s_0) \models p &\Leftrightarrow& s_0 \in \pi(p) \\  
(M, s_0) \models \lnot \varphi &\Leftrightarrow& (M, s_0) \nmodels \lnot \varphi\\
(M, s_0) \models \varphi \land \psi &\Leftrightarrow& (M, s_0) \models \varphi \mbox{ and  } (M, s_0) \models \psi \\
(M, s_0) \models EX\varphi &\Leftrightarrow& \exists \mbox{ a path } s_0, s_1, s_2, ... \mbox{ such that } (M, s_1) \models \varphi \\
(M, s_0) \models EG\varphi &\Leftrightarrow& \exists \mbox{ a path } s_0, s_1, s_2, ... \mbox{ such that } (M, s_i) \models \varphi \quad \forall i \geq 0\\
(M, s_0) \models E(\varphi U\psi) &\Leftrightarrow&  \exists \mbox{ a path } s_0, s_1, s_2, ... \mbox{ for which } \exists i \geq 0 \mbox{ such that }  (M, s_i) \models \psi \\ && \mbox{ and }  (M, s_j) \models \varphi \quad \forall 0 \leq j < i
\end{eqnarray*}
\end{definition}
The quantifiers allow for more properties to be expressed, for example:
\begin{itemize}

\item $EF(AG($deadlocked$))$

It is possibly to reach a point where the process will be permanently deadlocked.

\item $AG(EX($reboot$))$ 

From any state it is possible to reboot the system.


\end{itemize}
Again, some formulas can be expressed in LTL but not in CTL. For instance, the property that \textit{in every path where $p$ is true at some point then $q$ is also true at some point} is expressed in LTL as $Fp \rightarrow Fq $ but there is no equivalent CTL formula.
 The logic CTL* combines the syntax of LTL and CTL to provide a richer set of connectives. We will not go into any more details regarding CTL*, but we refer the reader to \cite{ctl*} for more information. 

\subsubsection{The Epistemic Logic CTLK}

In the case of multi-agent systems, we are interested in describing the system in terms of individual agents, and in particular their \textit{knowledge}. 

For this reason, we can add \cite{ctlk} a family of unary operators $K_i$ for $i = 1, ..., n$ to the modal connectives defined previously. Each $K_i$ will represent the intuitive notion of knowledge for agent $i$. 
This enables us to define the temporal-epistemic logics LTLK and CTLK, which are extensions of LTL and CTL, respectively. Here we leave out details about LTLK, as the practical applications we present later on only support CTLK. 



\begin{definition} 
The syntax of CTLK is defined by the following BNF: 
$$ \varphi := p \mid \lnot \varphi \mid \varphi \land \varphi \mid EX\varphi \mid EG\varphi \mid E(\varphi U \varphi) \mid  K_i\varphi \quad (i \in \{1, ..., n\})$$
\end{definition}  

We use interpreted systems (\ref{IS}) as semantic structures for CTLK. The satisfaction of a CTL formula on an interpreted system $IS$ is defined analogously to its satisfaction on a model $M$ whose worlds $W$ are the global states of $IS$, and whose relation function $R$ is the global transition function of $IS$. 
For example, $(IS, g_0) \models EX\varphi \mbox{  iff there is a run } r = g_0, g_1, g_2, ... \mbox{ of $IS$ such that } (IS, g_1) \models \varphi$.

The following definition completes the semantics of CTLK formulae:

\begin{definition}
Given an interpreted system $IS$, a global state $g$, an agent $i$ of $IS$, and a CTLK formula $\varphi$, we define
$$(IS, g) \models K_i \varphi \mbox{  iff  } \forall g' \in G, g \sim_i g' \Rightarrow (IS, g') \models \varphi$$

The connective $K_i$ expresses that agent $i$ \emph{knows} of the property $\varphi$ when the system's global state is $g$.

We extend the syntax and semantics of CTLK by adding two extra unary operators: $E$ (Everybody knows) and $C$ (Common knowledge), whose semantics are defined as follows:

\begin{eqnarray*}
(IS, g_0) \models E\varphi &\Leftrightarrow& (IS, g_0) \models K_i \varphi \quad \forall i = 1, ..., n \\
(IS, g_0) \models C\varphi &\Leftrightarrow&  (IS, g_0) \models \bigwedge^\infty_{k = 1} E^{(k)}\varphi \quad \\&& \mbox{ where } E^{(1)} = E \mbox{ and } E^{(j+1)} = EE^{(j)} \quad \forall j \geq 1
\end{eqnarray*}
\end{definition}


\subsection{Model Checking}

Model checking was briefly introduced in \ref{model_checking} as a automated verification technique, which can be used to check that a system $S$ satisfies a specification $P$. The technique involves representing $S$ as a logic system $L_S$ which captures all possible computations of $S$, and encoding the property $P$ as a temporal formula $\varphi_P$. 

The problem of verifying $P$ is then reduced to the problem of checking whether $L_S \vdash \varphi_P $. But we can now build a Kripke model $M_S = (W_S, R_S, \pi)$ such that $L_S$ is sound and complete over (the class of) $M_S$, so that $$L_S \vdash \varphi_P \Leftrightarrow M_S \models \varphi_P .$$ 
$M_S$ is the Kripke model representing all possible computations of $S$, i.e. $W_S$ contains all the possible computational states of the system and the relation $R_S$ represents all temporal transitions in the system. 

In the case of a multi-agent system as defined above, encoding $S$ as an interpreted systems of agents will satisfy the equivalence, and we can use CTLK to encode properties of the system to be checked. 

\subsubsection{Explicit Model Checking}

\label{explicit}
In this section we present a first approach to model checking, the so-called \textit{explicit} approach.

Suppose that we want to check that a multi-agent system $\Sigma$ satisfies a propery $P$. If $IS$ is an interpreted system representing $\Sigma$, and $\varphi$ the CTLK formula corresponding to $P$, we need to verify that $(IS, s_0) \models \varphi$, for all initial states $s_0 \in I$.

Algorithmically it is more efficient \cite{need ref} to compute the set of global states $\llbracket\varphi\rrbracket$ of $IS$ where $\varphi$ is true, and check that $I \subseteq \llbracket\varphi\rrbracket$. 
The following algorithm returns $\llbracket\varphi\rrbracket$ for any CTLK formula $\varphi$.


\begin{lstlisting}[mathescape]

function SAT($\varphi$) 
// returns $\llbracket\varphi\rrbracket$
if $\varphi = \top$: return $G$
if $\varphi = \bot$: return $\varnothing$
if $\varphi = p$: return $\pi(p)$ 
if $\varphi = \lnot\varphi_1$: return $G \setminus$SAT($\varphi_1$)
if $\varphi = \varphi_1\land\varphi_2$: return SAT($\varphi_1$)$\cap$SAT($\varphi_2$)
if $\varphi = EX\varphi_1$: return SAT$_{\mbox{EX}}(\varphi_1)$
if $\varphi = AF\varphi_1$: return SAT$_{\mbox{AF}}(\varphi_1)$
if $\varphi = E(\varphi_1 U\varphi_2 )$: return SAT$_{\mbox{EU}}(\varphi_1, \varphi_2)$
if $\varphi = K_i\varphi_1$: return SAT$_{\mbox{K}}(i, \varphi_1)$
if $\varphi = E\varphi_1$: return SAT$_{\mbox{E}}(\varphi_1)$
if $\varphi = C\varphi_1$: return SAT$_{\mbox{C}}(\varphi_1)$
end
$\\$
\end{lstlisting}

Notice that this covers all formulae $\varphi$, as $\{EX, AF, EU, K_i, E, C\}$ is a minimum set of connectives for CTLK. The respective auxilliary functions are defined below. Notation: for any global states $g_0, g_1$ of $IS$ we write $g_0 \rightarrow g_1$ iff $\exists a \in Act$ such that $\tau(g_0, a) = g_1$ (i.e. there is an run of $IS$ starting with $g_0, g_1, ...$).


\begin{lstlisting}[mathescape]

function SAT$_{\mbox{EX}}$($\varphi$)
// returns $\llbracket EX\varphi\rrbracket$
  X := {$g_0 \in G \mid g_0 \rightarrow g_1$ for some $g_1 \in $SAT($\varphi$) }
  return X
end
 
function SAT$_{\mbox{AF}}$($\varphi$)
// returns $\llbracket AF\varphi\rrbracket$
  X := $G$
  Y := SAT($\varphi$)
  repeat until X = Y:
    X := Y
    Y := Y $\cup$ $\{ g_0 \in G \mid $ for all $g_1 $ with $g_0 \rightarrow g_1$, $g_1 \in$ Y $\}$ 
  end
  return Y
end
  

function SAT$_{\mbox{EU}}$($\varphi_1$, $\varphi_2$)
// returns $\llbracket E(\varphi_1U\varphi_2)\rrbracket$
  W := SAT($\varphi$)
  X := $G$
  Y := SAT($\psi$)
  repeat until X = Y:
    X := Y
    Y := Y $\cup$ (W $\cap$ $\{g_0 \in G \mid \exists g_1 \in$ Y such that $g_0 \rightarrow g_1\}$)
  end
  return Y
end

function SAT$_{\mbox{K}}$($i$, $\varphi$)
// returns $\llbracket K_i\varphi\rrbracket$
  X := SAT($\lnot\varphi$)
  Y := $\{g_0 \in G \mid \exists g_1 \in$ X with $g_0 \sim_i g_1\}$
  return $G \setminus\mbox{Y}$
end

function SAT$_{\mbox{E}}$($\varphi$)
// returns $\llbracket E\varphi\rrbracket$
  X := SAT($\lnot\varphi$)
  Y := $\{g_0 \in G \mid \exists g_1 \in$ X with $g_0 \sim_i g_1$ for all $i = 1, ..., n\}$
  return $G \setminus\mbox{Y}$
end

function SAT$_{\mbox{C}}$($\varphi$)
// returns $\llbracket C\varphi\rrbracket$
  X := $G$
  Y := SAT($\lnot\varphi$)
  repeat until X = Y:
    X := Y
    Y := $\{g_0 \in G \mid \exists g_1 \in$ X with $g_0 \sim_i g_1$ for all $i = 1, ..., n\}$
  end
  return $G \setminus\mbox{Y}$
end
$\\$
\end{lstlisting}
The complexity of \texttt{SAT} is linear in the size of the model. However, the size of the model (i.e. the number of reachable states) grows exponentially in the number of variables used to describe the system $\Sigma$, therefore the explicit approach is not always viable in practice. This is the main difficulty in model checking and it is known as the \textit{state explosion problem}. 

Recall that in order to use the algorithm above we also need a way of computing the set of reachable states, which is rarely given explicitely. In most cases all that is given is the set $I$ of initial states of the system. The reachable state space can then be obtained by computing the fixed point of the transition relation function, as shown in the algorithm below:

\begin{lstlisting}[mathescape]

function compute_reach()
// returns the reachable state space $G$ 
  X := $\emptyset$
  Y := $I$
  repeat until X = Y
    X := Y
    Y := Y $\cup$  $\{ g_1 \in G \mid g_0 \rightarrow g_1 \mbox{ for some }g_0 \in X\}$
  end
  return Y
end
$$
\end{lstlisting}

In the next section we introduce a model checking technique aiming to improve the efficiency of the approach. 



\subsubsection{Symbolic Model Checking}

\textit{Symbolic model checking} is an approach to model checking which involves representing sets of states and functions between them as Boolean formulas. The algorithm presented in \ref{explicit} is then reduced to a series of operations on Boolean formulas. In this section we go through the process of encoding sets and functions as propositional formulas, and we explain how this encoding facilitates model checking.

\myparagraph{Symbolic Representation of Sets of States}
\label{symbolic}
\begin{figure}
    \centering
    \includegraphics[scale=0.55]{symbolic_model_checking_example.PNG}
    \caption{A model}
    \label{fig:model_example}
\end{figure}

We use an example \cite{logic_in_computer_science} to illustrate the encoding process.
Consider the model in Figure~\ref {fig:model_example}, representing a system with three states labelled $s_0, s_1, s_2$. 

We consider two propositional variables, namely $x_1$ and $x_2$.  
If $S$ is the whole state space (so $S = \{s_0, s_1, s_2\}$), we can represent subsets of $S$ using Boolean formulae, as shown in the following table:\\

\begin{tabular}{ l | r }
  set of states & representation by boolean formula \\ \hline
$\emptyset $&   $\bot$ \\
$\{ s_0\}$ & $x_1 \land \lnot x_2 $\\
$\{ s_1\}$ & $ \lnot x_1 \land x_2 $\\
$\{ s_2\}$ & $\lnot x_1 \land \lnot x_2 $\\
$\{ s_0, s_ 1\}$ & $(x_1 \land \lnot x_2) \lor (\lnot x_1 \land x_2)  $\\
$\{ s_1, s_2\}$ & $(\lnot x_1 \land  x_2) \lor (\lnot x_1 \land\lnot x_2)$\\
$\{ s_0, s_2\}$ & $(x_1 \land \lnot x_2) \lor (\lnot x_1 \land\lnot x_2) $\\
$\{ s_0, s_1, s_2\}$ & $(x_1 \land \lnot x_2) \lor (\lnot x_1 \land  x_2) \lor (\lnot x_1 \land\lnot x_2)$
\end{tabular}
\\
\\

Note that for this representation to be unambiguous, we must ensure that no two states satisfy the same set of Boolean variables. If this is the case, new variables can be added which will be used to differenciate between the ambiguous states.


\myparagraph{Symbolic Representation of a Transition Relation}

The transition relation $\rightarrow$ of a model is a subset of $S \times S$. Taking two copies of our set of propositional variables, we can then associate a Boolean formula to the transition relation, as follows. 

Firstly, notice that in the example above the transition relation $\rightarrow$ of the model is $$\{(s_0, s_1), (s_1, s_2), (s_2, s_0), (s_2, s_2)\}.$$
Our set of Boolean variables was $\{x_1, x_2\}$; we now create a copy and use \textit{primed} variables to represent its elements. We get another set $\{x_1', x_2'\}$, which will be used to represent the \textit{next} state in our encoding of the transition relation.
Now, for each pair element in the set, we take the conjunction of the Boolean representation of each state in the pair, the first one using the original set of variables, the second the primed set. For example, with $(s_0, s_1)$ is associated the formula $(\lnot x_1 \land \lnot x_2) \land (\lnot x'_1 \land \lnot x'_2)$ (using the Boolean representation for $s_0, s_1$ derived above -- see the table).

As in the representation of sets of states, we represent sets of pairs by taking the conjunction of the representation of each pair element. Thus we compute the representation of $\rightarrow$ to be $(\lnot x_1 \land \lnot x_2 \land \lnot x'_1 \land \lnot x'_2 ) \lor (\lnot x_1 \land \lnot x_2 \land x'_1 \land \lnot x'_2 ) \lor ( x_1 \land \lnot x_2 \land \lnot x'_1 \land  x'_2 ) \lor (\lnot x_1 \land x_2 \land \lnot x'_1 \land \lnot x'_2 )$.

\myparagraph{The functions $\pre_\exists$ and $\pre_\forall$}

We have seen that the explicit model checking algorithm relies on two functions on sets of states, $$\mbox{pre}_{\exists} (X) = \{g_1 \in G \mid \exists g_0 \in X \mbox{ with } g_0 \rightarrow g_1\}$$ and $$\mbox{pre}_\forall (X) = \{g_1 \in G \mid\mbox{ if }g_0 \rightarrow g_1,\mbox{ then }g_0 \in X\}.$$ We therefore need a way of implementing those using the symbolic representations of sets of states introduced above. First note that $$\mbox{pre}_\forall (X) = G \setminus \mbox{pre}_{\exists} (G \setminus X),$$ so it is enough to have an algorithm for computing $\mbox{pre}_{\exists}(X)$ on a given set~$X$. Let us first recall a basic definition about Boolean functions:

\begin{definition}
\label{exist}
Let $f$ be a Boolean function. The \textit{conditioning} of $f$ on a variable $x$, written $f|_x$, is the Boolean function obtained by changing $x$ to $\top$ in $f$, and similarly $f|_{\lnot x}$
is obtained from $f$ by setting $x$ to $\bot$.
The \textit{existential quantification} of $f$ with respect to $x$, denoted $\exists x f$, is used to relax the constraint on variable $x$ in $f$, and it is defined by $$\exists x f = f|_x \lor f|_{\lnot x}.$$
\end{definition}

Suppose now that $X \subseteq G$ is a set of states encoded by function $f_X$, and that the transition relation $\rightarrow$ of the system is encoded by function $f_\rightarrow$. The algorithm has the following steps:
\begin{enumerate}
\item Rename variables in $f_X$ to their primed counterparts, and call the resulting function $f_{X'}$. 
\item Compute $f_{\rightarrow X'} = f_{X'} \land f_\rightarrow$.
\item Existentially quantify all primed variables away from $f_{\rightarrow X'}$, i.e. compute $\exists x_1' ... \exists x_n' f_{\rightarrow X'}.$ The resulting function is $f_{\mbox{pre}_\exists(X)}$, the symbolic representation of $\mbox{pre}_\exists(X)$.
\end{enumerate}
In the \texttt{compute\_reach} algorithm above, we use a different operator, which computes the set of states \textit{following} $X$ in the transition relation. This can be encoded in terms of Boolean functions in a very similar way: start with the Boolean encoding of $X$, conjoin with the encoding of the transition relation, existentially quantify all non-primed variables, and ``un-prime" the variables in the resulting function.

\myparagraph{The case of Multi-Agent Systems}

Notice that the procedure described above is only valid for global states and does not distinguish between different agents. 
In the case of multi-agent systems, we can encode the local states for agents using the same method as for states in the general case, making sure to use a different set of variable for each agent. A Boolean representation for a global state in the system will then be the conjunction of the formulas for the local states of which it consists. The separate encoding of the states for each agent will also enable the agent protocols to be encoded independently.

Moreover, in order to successfully implement our model checking algorithm using symbolic expressions, we need a way of representing the agent accessibility relations $\sim_i$.
But each of these is a binary relation on states, i.e. a subset of $S \times S$. Hence we can use the exact same method as for the global transition relation. 
\\\\

How does this help us? Well, several techniques exist which allow us to represent these Boolean formulas in a very concise form. This is the topic of the next section.

\subsection{Representing Boolean functions}

It is important to make the point that the Boolean formulas computed in \ref{symbolic} can be regarded as Boolean \textit{functions}, i.e. functions $\{0, 1\}^n \rightarrow \{0, 1\}$ for some $n$. For example, the formula $x_1 \land \lnot x_2$ is associated to the function $f(x_1, x_2) = x_1 \land \lnot x_2$.

In this section we introduce various representations of Boolean functions using directed acyclic graphs (DAG). 

\subsubsection{Ordered Binary Decision Diagrams}

One the most basic ways of representing a Boolean function is by using a \textit{binary decision tree} (BDT). A BDT is a binary tree where we label non-terminal nodes with Boolean variables $x_1, x_2, ...$ and terminal nodes with the values 0 and 1. Each non-terminal node has two outgoing edges, one solid and one dashed (one for each value of the variable the node is labelled with). An example is shown in Figure~\ref{fig:BDT}. Note that the initial ordering of the variables affects the resulting BDT; for convenience, we write $[x_1, x_2, ..., x_n]$ to denote the order $x_1 < x_2 < ... < x_n$.


BDTs are a relatively inefficient way of storing Boolean formulas, since a BDT for a formula with $n$ variables has $2^{n+1} - 1$ nodes. Fortunately, a BDT can be reduced to an \textit{ordered binary decision diagram} (OBDD), a much more compact data structure.

\begin{figure}
\centering
\begin{tikzpicture}

	\node[circle, draw] (x1) {$x_1$}	 [sibling distance=27mm]
		child {node[circle,draw] (x21) {$x_2$} [sibling distance=14mm][dashed]
			child {node[rectangle, solid, draw] (1) {0} [dashed]}
			child {node[rectangle, solid, draw] (01) {0} [solid]}}
		child {node[circle,draw] (x22) {$x_2$} [sibling distance=14mm][solid]
			child {node[rectangle, solid, draw] (02) {1} [dashed]}
			child {node[rectangle, solid, draw] (03) {0} [solid]}};
\end{tikzpicture}
	\caption{BDT for $f(x_1, x_2) = x_1 \land \lnot x_2$ under the ordering $[x_1, x_2]$}
\label{fig:BDT}
\end{figure}

\myparagraph{The Reduction Algorithm} 

\noindent There are three steps in the reduction of BDTs to OBDDs:
\begin{enumerate}
\item Removal of duplicate terminals: we merge all the 0-nodes and 1-nodes into two unique terminal nodes
\item Removal of redundant tests: if both outgoing edges of a node $n$ point to the same node $m$, we remove $n$ from the graph, sending all its incoming edges directly to $m$ 
\item Removal of duplicate non-terminals: we merge any two subtrees with identical BDD structure
\end{enumerate}
Steps (2) and (3) are repeatedly applied until no further reduction is possible, and the resulting diagram is said to be a reduced ordered binary decision diagram. 

[here need example of using reduction algorithm]

The following key result makes the use of OBDDs viable in practice: 

\begin{theorem} \cite{bryant}
\label{obdd_uniqueness}
If $f$ is a Boolean function over the variables $x_1, ..., x_n$, then the OBDD representing $f$ is unique, up to the order of $x_1, ..., x_n$ chosen.
\end{theorem}

The immediate consequence of Theorem~\ref{obdd_uniqueness} is that one can easily compare two Boolean functions by comparing their respective OBDDs (provided both OBDDs have the same variable order). 

Another important observation to make is that OBDDs resulting from two different variable orders may present a \textit{significant }difference in size, and therefore a large amount of work has been done in the search for suitable variable orders. 

OBDDs help us to manipulate Boolean functions with a high number of variables, allowing us to use our model checking algorithm (\ref{explicit}) on systems with much larger state-spaces, which has led researchers in the past 15 years to explore various graph-based representations of Boolean functions.

\noindent\textit{Note to the reader: throughout this report we often drop the `O' and refer to OBDDs as BDDs.}

\subsubsection{A Knowledge Compilation Map}

In computer science, the field of \textit{knowledge compilation} is concerned with finding compact and efficient representations of propositional knowledge bases (such as symbolic representations of state-spaces, cf \ref{symbolic}).
 Such a representation is refered to as a \textit{target compilation language}, of which BDTs and OBDDs are examples. 
 
 The main properties that we look for in a target compilation language are the canonicity of the representation, the succinctness of the representation, and a polynomial-time complexity for queries and operations on the language. 
 These properties will ensure that we can safely rely on a particular language for our model checking algorithm. 
 
 OBDDs satisfy these two properties (with succintness remaining highly dependent on the variable order), and in fact a number of model checkers use them for state-space representation (e.g. NuSMV, MCMAS). 
 
 In 2002, A. Darwiche and P. Marquis published \cite{compilation_map} a comparative analysis of most existing DAG-based target compilation languages in terms of their succintness and the polytime operations they support. They show that all of these languages are subsets of a broad language called \textit{negation normal form} (NNF). 

\begin{definition}
A sentence in \textit{NNF} is a rooted directed acyclic graph where each leaf node is labelled with $\top$, $\bot$, $X$ or $\lnot X$ for some propositional variable $X$, and each internal node is labelled with $\land$ or $\lor$ and can have arbitrarily many children. 

\end{definition} 

\noindent Remark: OBDDs are NNF sentences. Figure~\ref{nnf_obdd} shows an OBDD and its corresponding NNF sentence.  

\begin{figure}
\centering
\begin{tikzpicture}


	\node[circle, draw] (X) at (0, 5){$X$};
	\node[circle, draw] (Y) {$Y$};

\end{tikzpicture}
\qquad\quad
\begin{tikzpicture}
	[level 1/.style={sibling distance=34mm},
	 level 2/.style={sibling distance=17mm},
	 level 3/.style={sibling distance=40mm},
	 level 4/.style={sibling distance=12mm},
	 level 5/.style={sibling distance=23mm},
	 level 6/.style={sibling distance=9mm}]
	\node[circle, draw] (or1) {$\lor$}	 
		child {node[circle,draw] (and1) {$\land$} [solid]
			child {node[rectangle, solid, draw] (X1) {$\lnot X$}[solid]}
			child {node[circle, solid, draw] (or2) {$\lor$} [solid]
				child {node[circle,draw] (and3) {$\land$} [solid]
					child {node[rectangle, solid, draw] (Y1) {$\lnot Y$}[solid]}
					child {node[circle, solid, draw] (or5) {$\lor$} [solid]
						child {node[circle,draw] (and5) {$\land$} [solid]
							child {node[rectangle, solid, draw] (Z1) {$\lnot Z$}[solid]}
							child {node[rectangle, solid, draw] (T2) {$\top$} [solid]} 	
						}
						child {node[circle,draw] (and6) {$\land$} [solid]
							child {node[rectangle, solid, draw] (Z2) {$Z$}[solid]}
							child {node[rectangle, solid, draw] (F1) {$\bot$} [solid]} 	
						}
					}
				}
				child {node[circle,draw] (and4) {$\land$} [solid]
					child {node[rectangle, solid, draw] (Y2) {$Y$}[solid]}
					child {node[circle, solid, draw] (or6) {$\lor$} [solid]
						child {node[circle,draw] (and7) {$\land$} [solid]
							child {node[rectangle, solid, draw] (Z3) {$\lnot Z$}[solid]}
							child {node[rectangle, solid, draw] (F2) {$\bot$} [solid]} 	
						}
						child {node[circle,draw] (and8) {$\land$} [solid]
							child {node[rectangle, solid, draw] (Z2) {$Z$}[solid]}
							child {node[rectangle, solid, draw] (T2) {$\top$} [solid]} 	
						}
					}
				}
			}
		}
		child {node[circle,draw] (and2) {$\land$} [solid]
			child {node[rectangle, solid, draw] (X2) {$X$} [solid]}
			child {node[rectangle, solid, draw] (T1) {$\top$} [solid]}
		};
\end{tikzpicture}

	\caption{An OBDD (left) and its corresponding NNF sentence (right). Although the former seems much more compact, the difference in size is only linear and both representations are essentially the same. TODO sort this oout}
\label{nnf_obdd}

\end{figure}
 
 
 
\subsubsection{Project Directions}

 This ``knowledge compilation map" was the starting point for this project, whose initial objective was to investigate new target compilation languages suitable for application to model checking, seeking a potential improvement on BDDs (which constitute so far the ``industry standard").

 Taking into account the various criteria for suitable representations described above, we searched the literature aiming to find a good candidate to replace BDDs. 

After considering a number of different representations including tree-of-BDDs \cite{trees_of_bdds}, BDD-trees \cite{bdd_trees}, and several other subsets of NNF described in \cite{compilation_map}, we opted for a relatively novel target compilation language called \textit{sentential decision diagram} (SDD). 

SDDs are a subset of NNF, possess the required properties, and had not yet been experimented with in the context of model checking and state-space representation. 
Moreover, some of the experimental results presented in \cite{sdd_1} and \cite{sdd_3} demonstrate that SDDs can lead to a significant improvement on BDDs in terms of computation time and memory usage. 

In the next section we present SDDs in full details, delaying our own experimental results to the following chapters.


\subsection{Sentential Decision Diagrams}

Most of the content in that section is taken from the work of A. Darwiche in \cite{sdd_1}, the first paper written on SDDs. 

\subsubsection{Preliminaries}

To define SDDs formally we must start with some preliminary definitions and results related to Boolean functions. 

\begin{definition} 
We say that a function $f$ \textit{essentially depends} on a variable $x$ iff $f|_{x} \neq f|_{\lnot x}$, and we write $f(X)$ if $f$ essentially depends on variables in $X$ only.
\end{definition}

\noindent Notation: we also write $f(X, Y)$ if $f(Z)$ and $X$, $Y$ are sets forming a partition of $Z$.

The following definition is the basis for the construction of SDDs:

\begin{definition} (Decompositions and partitions)
An \textit{($X$,$Y$)-decomposition} of a function $f(X, Y)$ is a set of pairs $\{(p_1, s_1),...,(p_n, s_n)\}$ such that $$ f = (p_1(X) \land s_1(Y)) \lor ... \lor (p_n(X) \land s_n(Y)).$$
The decomposition is said to be \textit{strongly deterministic} on $X$ if $p_i(X) \land p_j(X) = \bot$ for $i \neq j$. In this case, each ordered pair $(p_i, s_i)$ in the decomposition is called an \textit{element}, each $p_i$ a \textit{prime} and each $s_i$ a \textit{sub}.

Let $\alpha = \{(p_1, s_1),...,(p_n, s_n)\}$ be an ($X,Y$)-decomposition, and suppose $\alpha$ is strongly deterministic on $X$.
Then $\alpha$ is called an $X$-\textit{partition} iff its primes form a partition (i.e primes are pairwise mutually exclusive, each prime is consistent, and the disjunction of all primes is valid).
We say that $\alpha$ is \textit{compressed} if $s_i \neq s_j$ for $i \neq j$.
\end{definition}

\begin{example} 
Let $f(x, y, z) = (x \land y) \lor (x \land z)$. Then $\alpha = \{(x, y\lor z)\}$ is an $(\{x\}, \{y, z\})$-decomposition of $f$ which is strongly deterministic (as there is only one prime). It is however not an $\{x\}$-partition, but $\beta = \{(x, y\lor z), (\lnot x, \bot)\}$ is, since $x, \lnot x$ form a partition. Note that $\beta$ is compressed. 
\end{example}

\noindent Remark that in an $X$-partition $\bot$ can never be prime, and if $\top$ is prime then it is the only prime. Moreover primes determine subs, so two $X$-partitions are different iff they contain distinct primes.

\begin{theorem}
\label{theorem_apply}
Let $\circ$ be a Boolean operator and let $\{(p_1, s_1),...,(p_n, s_n)\}$ and $\{(q_1, r_1),...,(q_m, r_m)\}$  be $X$-partitions of Boolean functions $f(X, Y)$ and $g(X, Y)$ respectively. Then $$\{(p_i \land q_j, s_i \circ r_j) \mid  p_i \land q_j  \neq \bot \}$$ is an $X$-partition of $f \circ g$.
\end{theorem}
\begin{proof}
 Since $p_1, ..., p_n$ and $q_1, ..., q_m$ are partitions, the $(p_i \land q_j)$ also form a partition for $i = 1, ..., n$, $j = 1, ..., m$ and $p_i \land q_j \neq \bot$. 
 
To do: finish proof 
\end{proof}

As we see later on, an important consequence of Theorem~\ref{theorem_apply} is the polynomial time operations available on SDDs. Canonicity of SDDs is due to the following result:

\begin{theorem}
\label{unique_compressed}
A function $f(X, Y)$ has exaclty one compressed $X$-partition.
\end{theorem}
\begin{proof}
proof
\end{proof}

\myparagraph{Vtrees}

Vtrees (for ``variable trees") are to SDDs what variable orders are to BDDs. A vtree completely determines the structure of an SDD, so they are crucial to the viability of SDDs in practice.

\begin{definition}
A \textit{vtree} for variables $X$ is a full binary tree whose leaves are in one-to-one correspondence with the  variables in $X$. We will often not distinguish between a vtree node $v$ and the subtree rooted at $v$, and the left and right children of a node $v$ will be denoted $v^l$ and $v^r$, respectively.
\end{definition}

\begin{figure}
\centering
\includegraphics[scale=0.5]{balancedABCD.png}
\qquad
\includegraphics[scale=0.5]{rightlinearABCD.png}


\caption{Two vtrees for $X = \{A, B, C, D\}$ To Do sort out nodes IDs}
\label{fig:vtree_examples} 

\end{figure}
Note that a vtree on a set $X$ is stronger than a total order of the variables in $X$. Figure~\ref{fig:vtree_examples} shows two distinct vtrees which induce the same variable order. 

\subsubsection{Definition and Construction}

The construction of an SDD for a Boolean function $f$ with respect to a vtree $v$ is done by a recursive algorithm on the children nodes of $v$. 

Let $v$ be the vtree on the left of Figure~\ref{fig:vtree_examples}, and let $$f(A, B, C, D) = (A \land B) \lor (B \land C) \lor (C \land D).$$

The decomposition of $f$ at the vtree node $v$ goes as follows: we split the variables in $X$ into two subsets by separating variables in $v^l$ from those in $v^r$: we obtain $\{A, B\}$ and $\{C, D\}$; we take the unique compressed $\{A, B\}$-partition of $f$, namely $\alpha = \{(\lnot B , C\land D), (\lnot A \land B, C), (A \land B, \top)\}$. The SDD for $f$ is the decomposition obtained by further decomposing the primes of $\alpha$ at $v^l$ and its subs at $v^r$. We continue this recursively until all elements consist of literals or constants. 

\myparagraph{Graphical Representation of SDDs}

The SDD constructed for function $f$ is represented on the left in Figure~\ref{fig:sddexample1}. On the right is the SDD for $f$ constructed with respect to the vtree on the right in Figure~\ref{fig:vtree_examples}.

A decomposition is represented by a circle with outgoing edges pointing to its elements, and an element is represented by a pair of boxes where the left box represents the prime and the right box represents the sub. If one of them is another decomposition, we leave the box empty and draw an edge pointing to the circle node representing it.

\begin{figure}
\centering
\includegraphics[scale=0.3]{balancedfirstexample.png}
\includegraphics[scale=0.3]{rightlinearfirstexample.png}

\caption{SDDs for $f = (A \land B) \lor (B \land C) \lor (C \land D)$ corresponding to the vtrees in Figure~\ref{fig:vtree_examples}. Notice that identical SDD nodes have been merged.}
\label{fig:sddexample1} 
\end{figure}

The next two definitions formally define the syntax and semantics of SDDs.

\begin{definition} (Syntax) Let $v$ be a vtree. $\alpha$ is an SDD that respects $v$ iff: 
\begin{itemize} 
\item $\alpha = \top$ or $\alpha = \bot$
\item $\alpha = X$ or $\alpha = \lnot X$, and $v$ is a leaf with variable $X$
\item $v$ is an internal node (i.e. it has children), and $\alpha$ is a partition  $\{(p_1, s_1), ..., (p_n, s_n)\}$ such that for all $i$, $p_i$ is an SDD that respects $v^l$ and $s_i$ is an SDD that respects $v^r$.
\end{itemize}
In the first two cases we say that $\alpha$ is \textit{terminal}, and in the third case $\alpha$ is called a \textit{decomposition}. For SDDs $\alpha$ and $\beta$, we write $\alpha = \beta$ iff they are \textit{syntactically equal}.
\end{definition}

\begin{definition} (Semantics)
Let $\alpha$ be an SDD. We use $\langle . \rangle$ to denote the mapping from SDDs to Boolean functions, and we define it inductively as follows: 
\begin{itemize} 
\item $\langle \top \rangle = \top$ and  $\langle \bot \rangle = \bot$
\item $\langle X \rangle = X$ and $\langle \lnot X \rangle = \lnot X$, for all variables $X$
\item $\langle\{(p_1, s_1), ..., (p_n, s_n)\}\rangle = \bigvee_{i = 1}^n \langle p_i \rangle \land \langle s_i \rangle  $
\end{itemize}
We say two SDDs $\alpha$ and $\beta$ are \textit{equivalent} (written $\alpha \equiv \beta$) if $\langle \alpha \rangle = \langle \beta \rangle$.

\end{definition}

\subsubsection{Canonicity and Operations} 

It is obvious that if SDDs $\alpha$ and $\beta$ are equal, then they are equivalent. We would however like 
to impose conditions on the construction of $\alpha$ and $\beta$  so that $\alpha \equiv \beta \Rightarrow \alpha = \beta$, which would make SDDs a \textit{canonical }representation, a crucial property.
We begin with a few definitions and lemmas.

\begin{definition}
Let $f$ be a non-trivial Boolean function. We say $f$ \textit{essentially depends} on vtree node $v$ if $v$ is a deepest node that includes all variables that $f$ essentially depends on. 
\end{definition}

\begin{lemma}
A non-trivial function essentially depends on exactly one vtree node.
\label{unique_vtree_function}
\end{lemma} 
 
\begin{definition}
An SDD is \textit{compressed} iff all its decompositions are compressed. It is \textit{trimmed} iff it does not have decompositions of the form $\{(\top, \alpha)\}$ or $\{(\alpha, \top), (\lnot\alpha, \bot)\}$ for some SDD $\alpha$.
\end{definition}


These two properties are very accessible. An SDD is compressed as long as all $X$-partitions used during its construction are compressed, and it can be trimmed by traversing it bottom-up and replacing decompositions of the form  $\{(\top, \alpha)\}$ or $\{(\alpha, \top), (\lnot\alpha, \bot)\}$ by $\alpha$. 
Theorem~\ref{canonicity} below shows that they are in fact sufficient for the representation to be canonical. To prove it we first need another lemma. 

\begin{lemma}
Suppose $\alpha$ is a non-trivial, compressed and trimmed SDD. Then $\alpha$ respects a unique vtree node $v$, which is the unique node that the Boolean function $f = \langle \alpha \rangle$ essentially depends on.
\label{unique_vtree_sdd}
\end{lemma}

\begin{theorem} 
\label{canonicity}
Let $\alpha$ and $\beta$ be compressed and trimmed SDDs. Then $$\alpha = \beta \Leftrightarrow \alpha \equiv \beta.$$
\end{theorem}

\begin{proof}
$(\Rightarrow)$ is clear. For $(\Leftarrow)$, suppose that $\alpha \equiv \beta$ and let $f = \langle \alpha \rangle = \langle \beta \rangle.$ If $f$ is constant, then $\alpha$ and $\beta$ are trivial SDDs, therefore they are equal.
Suppose  now that $f$ is non-trivial, and let $v$ be the vtree node that $f$ essentially depends on (it is unique by Lemma~\ref{unique_vtree_function}). Then by Lemma~\ref{unique_vtree_sdd}, $\alpha$ and $\beta$ respect $v$. 
We continue the proof by structural induction on $v$. 

If $v$ is a leaf, then $\alpha$ and $\beta$ are terminals. But $f$ is non-trivial so $\alpha$ and $\beta$ are equivalent literals, and so they must be equal. 
Suppose now that $v$ is internal, and that the theorem holds for $v^l$ and $v^r$. Let $X$ be the variables in $v^l$ and $Y$ be the variables in $v^r$. 
Write $\alpha = \{(p_1,s_1), ..., (p_n,s_n)\}$  and $\beta = \{(q_1,r_1), ..., (q_m,r_m)\}$, where the $p_i, q_j$ are SDDs with respect to $v^l$ and the $s_i, r_j$ are SDDs with respect to $v^r$. Then  $\{(\langle p_1 \rangle,\langle s_1\rangle), ..., (\langle p_n\rangle,\langle s_n\rangle)\}$ and  $\{(\langle q_1 \rangle,\langle r_1\rangle), ..., (\langle q_n\rangle,\langle r_n\rangle)\}$ are $X$-partitions of $f$, and they are compressed since $\alpha$ and $\beta$ are compressed SDDs. So by Theorem~\ref{unique_compressed}, they are the same. So $n = m$, and for all $i$ we have $\langle p_i\rangle = \langle q_i \rangle $ and $\langle s_i \rangle = \langle r_i\rangle$, possibly after reordering. Then by definition $p_i \equiv q_i$ and $s_i \equiv r_i$, which by induction implies that $p_i = q_i$ and $s_i = r_i$. So $\alpha = \beta$.
\end{proof}

\myparagraph{Operations on SDDs} 

We start right away by giving the pseudo-code for the \texttt{apply} algorithm on SDDs, which combines two SDDs $\alpha$ and $\beta$ using a Boolean operator $\circ$, provided they respect the same vtree node.
 
\begin{lstlisting}[mathescape]

 1 function apply($\alpha$, $\beta$, $\circ$)
 2  if $\alpha$ and $\beta$ are constants or literals
 3    return $\alpha \circ \beta$   
 4  else if cache($\alpha$, $\beta$, $\circ$) != null
 5    return cache($\alpha$, $\beta$, $\circ$)
 6  else
 7    $\gamma$ = {}
 8    for all elements ($p_i$, $s_i$) in $\alpha$
 9      for all elements ($q_j$, $r_j$) in $\beta$ 
10        $p$ = apply($p_i$, $q_j$, $\land$)
11        if p != $\bot$
12          $s$ = apply($s_i$, $r_j$, $\circ$)
13          if $\nexists$ element ($q$, $s$) in $\gamma$
14            add ($p$, $s$) to $\gamma$
15          else 
16            add (apply($p$, $q$, $\lor$), s) to $\gamma$
17          end if
18        end if
19      end for
20    end for
21    return cache($\alpha$, $\beta$, $\circ$) = $\gamma$
22  end if
23 end function
$$
\end{lstlisting}

If $\alpha$ and $\beta$ are compressed, then this algorithm returns a compressed SDD for $\alpha \circ \beta$. 
Theorem~\ref{theorem_apply} ensures that \texttt{apply($\alpha$, $\beta$, $\circ$)} is in fact an SDD, while the if condition on line 13 checks that its subs are distinct, thereby making it a compressed SDD. The use of the \texttt{cache} in the pseudo-code emphasises the fact that any implementation of \texttt{apply} could be significantly improved by keeping the set of computed SDDs in a cache in memory. 

The \texttt{apply} algorithm to compose SDDs $\alpha$ and $\beta$ takes time $O(|\alpha||\beta|)$, which implies that conjunction, disjunction and negation of SDDs can all be done in polynomial time.

To do: Need to write something about the \texttt{condition()} algorithm which comes up a lot in model checking.

\subsubsection{OBDDs are SDDs}
\label{bdds_are_sdds}
To end the background section of this report, we would like to  focus on the fact that \textit{OBDDs are SDDs}, i.e. if we regard them as subsets of NNF, then OBDD $\subseteq$ SDD. We first give a simple explanation for why this is the case. 

Consider an SDD $\alpha$ respecting a vtree $v$ such that every left child is a leaf. Such a vtree is said to be \textit{right-linear}, and every decomposition in $\alpha$ will be of the form $\{(A, \beta_1), (\lnot A, \beta_2)\}$ for some variable $A$ and SDDs $\beta_1, \beta_2$. 
But this is the same decomposition as that occuring in a BDD at the node labelled with $A$.

Suppose now that $B$ is a BDD for function $f$, and that $v$ is the \textit{only} right-linear vtree inducing the variable order for $B$. Then the SDD for $f$ respecting $v$ is \textit{syntactically equal} to $B$, when both are regarded as NNF sentences. Hence there is a one-to-one correspondence between BDDs and SDDs respecting right-linear vtrees. 
Figure~\ref{fig:bdds_are_sdds} illustrate this with an example. 

\begin{figure}

\begin{tikzpicture}

	[level 1/.style={sibling distance=33mm},
	 level 2/.style={sibling distance=23mm},
	 level 3/.style={sibling distance=16mm}]
	\node[circle, draw] (X) {$X$}	 
		child {node[circle, solid, draw] (Y) {$Y$} [dashed]
			child {node[circle, solid, draw] (Z1) {$Z$} [dashed]
				child {node[rectangle, solid, draw] (0) {0} [solid]}
				child {node[rectangle, solid, draw] (1) {1} [dashed]}}
			child {node[circle, solid, draw] (Z2) {$Z$} [solid]}};
	\draw[solid] (X) -- (1); 
\end{tikzpicture}

%\includegraphics[scale=•]{•}

\caption{The BDD for $f = $ with variable order $[]$ (left), and the SDD respecting the right-linear vtree inducing the same order (right)}
\label{fig:bdds_are_sdds}
\end{figure}

A very general consequence of this is that any application of BDDs in computer science could be implemented using SDDs instead, as it would retain the attractive properties of BDDs while benefiting from the potential reduction in size that SDDs present. In the more specific case of this project, we will see in a later chapter that this correspondence was extremely useful in helping us compare the efficiency of SDDs with that of BDDs in model checking: by restricting our SDD implementation to right-linear vtrees, we were able to compare both programs more accurately by taking into account the overhead due to the difference in \textit{implementation} (as opposed to a difference in size between the two structures -- we knew these were the same!).


\section{Implementation}

This chapter is devoted to the implementation of the model checker itself. We start by giving an overview of the existing code base and libraries upon which it relies, before presenting the important design decisions and algorithms, and finally discussing the challenges that occured during the implementation of this new model checker. 

\subsection{Preliminary I: The SDD Package}

\subsubsection{Description}

\textit{The SDD Package} is a C library which can be used to create and manipulate SDDs. It was developed at UCLA by the Automated Reasoning group, who first introduced SDDs. Its current version is 1.1.  


The SDD Package API contains most of the functions required for the use of SDDs in model checking. This includes basic manipulations such as conjunction, disjunction, and negation of Boolean functions represented by SDDs, conditioning a function on a literal, and quantifying out variables (the SDD equivalent of $\exists$ and $\forall$). Additionally, the API makes possible a number of operations on vtrees, including \textit{rotating }and \textit{swapping} (these are crucial for navigating the space of vtrees in the context of SDD minimisation, see below for details). 

The \textit{SDD manager} is the focal point for all the SDD operations in the program. It is there to ensure that all SDDs in the program have been built with respect to the same vtree, and it handles SDD conversions in the case of a vtree modification. It also gives the user access to a number of statistics, helpful for tracking a program's memory usage or the size of some SDD nodes. 

\subsubsection{Standard Vtrees}
\label{std_vtrees}

Four different ``classes" of vtrees are pre-implemented in the SDD package, i.e. can be created in one function call, provided a variable order is supplied. 
They are:
\begin{itemize}
\item right-linear: all left children are leaves
\item left-linear: all right children are leaves
\item balanced: the vtree is a balanced binary tree, i.e. both children of a node have the same number of leaves (if total number is even)
\item vertical: every node has a leaf child, which is alternatively the right child or the left child
\end{itemize}
Throughout this report we refer to these as the \textit{standard} vtrees. An example of each is given in Figure~\ref{fig:std_vtrees_example}.

\begin{figure}
\centering
\includegraphics[scale=0.4]{rightlinearABCD.png}
\quad
\includegraphics[scale=0.4]{balancedABCD.png}
\quad
\includegraphics[scale=0.4]{leftlinearABCD.png}
\quad
\includegraphics[scale=0.4]{verticalABCDE.png}
\caption{The four standard vtrees, in order: right-linear, balanced, left-linear, and vertical.}
\label{fig:std_vtrees_example}
\end{figure}

\subsubsection{Dynamic SDD Minimisation}
\label{sdd_dao_algo}
One particularly important feature of the SDD Package is \textit{dynamic SDD minimisation}. When enabled, this feature automatically attempts to minimise the size of a manager's SDD nodes by searching for a better vtree. Unfortunately this is not always a more efficient solution as searching the space of vtrees is a very lengthy process: there are $\frac{(2n -2)!}{(n-1)!}$ distinct vtrees over $n$ variables!

An vtree search algorithm was proposed in \cite{sdd_3} for more efficient dynamic minimisation of SDDs. This algorithm relies on three standard binary tree operations: left-rotation, right-rotation, and swap; these operations are sufficient for navigating the space of all vtrees \cite{knuth}.
The following example shows in what way these operations affect vtrees: 
\begin{example}
take a vtree with 4 nodes
\end{example}

We now give an informal description of the algorithm in \cite{ recursive calls on $v_l$ and $v_r$. We then consider two subtreesdd_3} (the one implemented in the SDD Package). We start with a vtree node $v$ (typically, the current vtree of the manager). The algorithm first makes two recursive calls on the children of $v$, ensuring that their structure is optimal. 
It then considers two vtree fragments of $v$, the \textit{l-vtree} and the \textit{r-vtree}. As shown on Figure~\ref{fig:lrvtrees}, each of these is a binary tree with 3 leaves (which might not be leaves in $v$). The algorithm attempts to compute the 24 distinct vtree fragments obtained by applying rotations and swaps to the l- and r-vtrees, and then selects the one leading to the best reduction in size for the SDD nodes depending on $v$. 

For each of the three vtree operations, users of the SDD package can set limits on the time needed to compute them, and the increase in size that they induce; this means that not all 24 vtree fragments above are necessarily computed. 

\subsubsection{Comparison with CUDD}
\label{comparison_with_cudd}

CUDD is the C$++$ BDD library used by MCMAS (see below) for BDD manipulation, and it is therefore our reference for all practical comparisons between SDDs and BDDs. 

Here we simply give an account of the differences between  CUDD and the SDD package, in order for our comparisons to be fair: we are interested in the relative efficiency of the data structures, not the packages, and therefore it is important that we take into account any differences in design and implementation. 

Fortunately, the libraries have a very similar flavour. For instance both are organised around a manager handling all internal operations. A few differences are worth mentioning:
\begin{itemize}
\item Both libraries have an automated garbage collection feature, and both are based on node reference counts. However the SDD Package requires users to manually increment and decrement the reference counts of each SDD node they create, whereas in CUDD this is optional (MCMAS doesn't use it). 
\item In the SDD Package the automated garbage collection and dynamic minimisation features are not independent, i.e. one cannot be enabled without the other being too. This means that when experimenting with SDDs without dynamic minimization, we find that a lot of unused nodes are kept in memory, sometimes preventing the program from generating any more SDDs in an efficient way (once the memory has run out, which happens relatively often in model checking when no garbage collection takes place). This is not the case in CUDD. 
\item An important difference in implementation is in the \texttt{exist()} function which relaxes the constraint on a variable $x$ in a function $f$, returning $f|_x \lor f|_{\lnot x}$. 
To do write more about exist
\end{itemize}
\label{gc}

\subsection{Preliminary II: MCMAS} 


MCMAS (Model Checker for Multi-Agent Systems) is a BDD-based model checker written in C$++$, which was developed, and is currently maintained at Imperial College in the Verification of Autonomous Systems group. It was specifically designed for the verification of multi-agent systems, and users can write system descriptions in a language called ISPL (Interpreted System Programming Language), whose syntax is very much inspired from the definition of interpreted systems (\ref{IS}). In order to better understand the implementation of the model checker, the reader is strongly advised to start by getting an overview of the main components in an ISPL file. For this purpose we supply a full example of an interpreted system encoded in ISPL: the \textit{Bit Transmission Protocol}, available in the appendix. ISPL is a fairly straightforward language, we therefore will not be spending any more time describing it here, but should anything remain unclear, the reader is refered the MCMAS User Manual \cite{mcmas}.



The model checker built for this project is (for a large part) based on the MCMAS code base. In this section we outline some of the MCMAS internal implementation details in order to help the reader understand the steps undertaken when replacing BDDs with SDDs. Moreover, MCMAS will be the basis of our BDDs vs. SDDs comparisons in the next chapters so it is important that the reader understand the different configuration options.

\subsubsection{Important Classes and Methods}
\label{MCMAS_steps}
The ISPL parser in MCMAS creates a model of the system using the following important classes: 
\begin{itemize}
\item \texttt{basic\_agent}: an agent in the system, consisting of a protocol, an evolution, a set of variables and a set of actions
\item \texttt{evolution\_line} and \texttt{protocol\_line}: a line in the evolution or in the protocol of an agent, consisting of a Boolean expression and an assignment or (respectively) an action
\item \texttt{bool\_expression} and \texttt{assignment}
\item \texttt{variable}, \texttt{basic\_type}, \texttt{int\_value}, \texttt{enum\_value}, \texttt{bool\_value},  \texttt{rangedint}, \texttt{atomic\_proposition}, \texttt{laction}: agent variables, their types and their values
\item \texttt{modal\_formula}: a CTLK formula to be checked in the model.
\end{itemize}
Throughout the model checking procedure the \textit{BDD parameters} are carried by the program and passed as argument to the various methods. They are encapsulated in a structure (\texttt{struct bdd\_parameters}) and contain all the important data required by the algorithm, in particular: 
\begin{itemize}
\item three vectors \texttt{v}, \texttt{pv}, and \texttt{a}, containing the Boolean variables used in the model for encoding states, next states, and actions respectively
\item a vector \texttt{vRT}, containing for each agent the BDD for its transition relation 
\item the BDD \texttt{in\_st} representing the set of initial states, and (once computed) the BDD \texttt{reach} for the reachable state space
\item a pointer to the BDD cache
\item a vector \texttt{is\_formulae} containing the \texttt{modal\_formula} objects for each CTLK formula to check
\end{itemize}
MCMAS is (quite literally) an implementation of the model checking algorithm described in \ref{explicit}. However, as a symbolic model checker it also contains procedures for encoding protocols, transition relations, sets of states and actions, etc., and in fact these constitute a very large part of the code. 
We list the main steps in the execution of the program:
\begin{enumerate}
\item Parse ISPL file, allocate variables and set up CUDD (see \ref{variable_allocation} below)
\item Compute the BDD for the global transition relation
\item Compute the BDD for the reachable state space 
\item For each modal formula $\varphi$, compute
\texttt{SAT($\varphi$)} and check that it is a subset of $I$
\item Output result and free memory
\end{enumerate}


\subsubsection{Variable Allocation}
\label{variable_allocation}
Variable allocation is the process of allocating manager variables (created with the manager) to agents and determining the various sets of variables needed for symbolic representation within each agent: state variables, primed state variables (a copy of the state variables representing the \textit{next }states), and action variables.

For each agent, MCMAS first computes the number of variables needed in each of the aforementioned sets by calls to the functions \texttt{state\_BDD\_length()} and \texttt{action\_BDD\_length()}.

The \texttt{basic\_agent} functions \texttt{allocate\_BDD\_2\_variables()} and \texttt{allocate\_BDD\_2\_actions()} are then used for variable allocation: they assign a portion of both \texttt{v} and \texttt{a} to each agent, giving them start and end \textit{indices}. Note that this forces all of an agent's state variables (and similarly, action variables) to be next to each other in \texttt{v} (and similarly, \texttt{a}). 

It may seem confusing that variable allocation happens before the user has been able to select a particular variable order, but in fact no actual variables have yet been allocated, only their position in the arrays. The user's choice will then affect the way the \textit{manager}'s variables are dispatched across \texttt{a}, \texttt{v}, and \texttt{pv}. 

In MCMAS, the user can choose between four \textit{standard} different variable orders. We take the time to present them here, not only because of the impact that this choice has on the overall performance, but also for reference in the future chapters (where we compare these orders with various SDD vtrees).

\subsubsection{Standard Variable Orders}
 \label{std_orderings}
Suppose MCMAS is running on an example requiring $n$ state variables denoted $x_1, ..., x_n$, and $m$ action variables denoted $a_1, ..., a_m$. By definition there are $n$ primed state variables (the next state is a state, so it can be represented with the $n$ state variables), these are denoted $x_1', ..., x_n'$. 
Suppose also that there are $k$ agents, and that for each $i$, agent $i$ has been allocated variables $x_{i_1}, ..., x_{i_{n_i}}$, $x_{i_1}', ..., x_{i_{n_i}}'$ and $a_{j_1}, ..., a_{j_{m_i}}$, for some $n_i, m_i \in \mathbb{N}$ and where $i_1, ..., i_{n_i}$ and $j_i, ..., j_{m_i}$ are sequences of consecutive integers.



The manager will then have $2n + m$ variables in total, and the following are the possible four ordering options with respect to which BDDs will be constructed throughout the process.
In options 2 to 4, variables are ordered using the agent order, so that we get an ordering of the form $$ [\mbox{variables for agent } 1, \mbox{variables for agent } 2, ...,  \mbox{variables for agent } k],$$ 
and the difference lies in the way variables are ordered within each agent set.
\begin{itemize}
\item Ordering option 1 (no interest in agent allocation):
$$[x_1, ..., x_n, x_1', ..., x_n', a_1, ..., a_m]$$
\item Ordering option 2 (variables for agent $i$):
$$
 [x_{i_1}, x_{i_1}', x_{i_2}, x_{i_2}',..., x_{i_{n_i}}, x_{i_{n_1}}', a_{i_1}, ..., a_{i_{m_1}}]
$$
\item Ordering option 3 (variables for agent $i$):
$$
 [x_{i_1}, ..., x_{i_{n_i}},a_{i_1}, ..., a_{i_{m_1}}, x_{i_1}', ..., x_{i_{n_1}}']
$$
\item Ordering option 4 (variables for agent $i$):
$$
 [x_{i_1}, ..., x_{i_{n_i}}, x_{i_1}', ..., x_{i_{n_1}}', a_{i_1}, ..., a_{i_{m_1}}]
$$
\end{itemize}

These different orderings can have a great impact on the size of the resulting BDD, and consequently on MCMAS computation times. It is generally accepted that the ordering option 2 is the best to use in model checking multi-agent systems. It is based on the concept of \textit{variable interleaving} in OBDDs \cite{interleaving}, and adapted to the concept of agents.


\subsubsection{Algebraic Decision Diagrams}

MCMAS supports bounded integer variables (e.g. \texttt{x : 0..3}), and allows Boolean conditions to be numeric identities (e.g. \texttt{(x > 2)} or \texttt{(x + y = 3)} for integer vars \texttt{x} and \texttt{y}). \mbox{variables for agent } i = 

If an agent has an integer variable with a large range of values, then the number of Boolean variables needed to represent its state is also large (if variable \texttt{x} has $n$ possible values, the corresponding agent needs at least $\log_2(n)$ state variables). 

To avoid this, MCMAS uses alternative data structures called \textit{algebraic decision diagrams} (ADD, \cite{add}) to represent these variables and expressions. The CUDD manager also handles ADDs, and the ADD variables needed are stored in global vectors \texttt{addv} and \texttt{addpv}.

As there is no SDD equivalent for ADDs (yet!), our model checker does not support examples containing numerical values and expressions.
Note that all these examples \textit{could} technically be implemented in ISPL so that our model checker supports them, by simply replacing an integer range by an \textit{enum} containing all the possible values that the variable can take: for example 
$$\texttt{x : 0..3} $$
could be declared as $$\texttt{x : \{zero, one, two, three\}}$$
and expressions such as $$\texttt{if (x > 1)}$$ could be translated to $$\texttt{if (x = two) or (x = three)}.$$
With ADDs being beyond the scope of this project, we did not study them further (in particular we did not look into the ADD reduction done by CUDD in the background), and therefore thought better not to implement this to keep the comparison fair between MCMAS and our model checker. 

\subsection{A Model Checker Based on SDDs}

\subsubsection{Adapting MCMAS}

We implemented a model checker for multi-agent systems entirely based on SDDs for symbolic representations. 

Our starting point was the MCMAS source code, in which we proceeded to replace every call to CUDD with equivalent calls to the SDD Package. Throughout, we managed the reference counts of our SDD nodes by calls to \texttt{sdd\_ref()} and 
\texttt{sdd\_deref()}. Note that this often requires the introduction of temporary nodes; specifically, this occurs when updating the value of a node based on its previous value. For instance, the CUDD statement 
\begin{verbatim}
// f, g are BDDs
f = f * g; 
\end{verbatim}
setting the value of $f$ to be the conjunction of the BDDs for $f$ and $g$, becomes
\begin{verbatim}
// f, g are SddNode*
SddNode* tmp;
f = sdd_conjoin(tmp = f, g, manager);
sdd_ref(f, manager);
sdd_deref(tmp, manager);
sdd_deref(g, manager); // (if this is the last time we use g)
\end{verbatim}
This is to ensure that the previous SDD node pointed to by $f$, which the code can no longer access, is garbage collected. As a more complete example we provide the code for the \texttt{compute\_reach()} function in appendix (To Do).

The main steps of our model checker are the same as those of MCMAS described in \ref{MCMAS_steps}, and therefore the important functions which had to be adapted for SDDs are: 
\begin{itemize}
\item the functions \texttt{encode\_protocol()}  and \texttt{encode\_evolution()} for each agent, used to compute the global transition relation
\item the \texttt{compute\_reach()} function for computing the SDD representing the global reachable state space
\item the \texttt{check\_formula()} method (the implementation of \texttt{SAT})
\end{itemize}

Additionally, new data types and functions had to be defined in order to deal with the SDD-specific aspects of the program. We replaced the MCMAS BDD parameters by our own \textit{SDD parameters} (see appendix) containing the results of the variable allocation process in three different vectors: \texttt{variable\_sdds}, \texttt{primed\_variable\_sdds}, \texttt{action\_variable\_sdds}.

\subsubsection{Existential Quantification}

Recall  (\ref{exist}) that the model checking algorithm relies on the functions $\mbox{pre}_\exists$ and $\mbox{pre}_\forall$, which themselves depend on existential quantification, or the process of relaxing the constraint on one or more variables in a function. 

In CUDD, the function responsible for existential variable quantification is \texttt{Cudd\_bddExistAbstract()}. This takes as input two BDDs $f$ and $c$, and returns the BDD for $f$ after existential quantification of all variables in $c$. The implementation of this function is based on the following recursive algorithm called \texttt{restrict}, originally described in \cite{unified_framework}: 

\begin{lstlisting}[mathescape]

function restrict(f, c):
// pre:$ \mbox{ c != } \bot$ and variables of c are all variables of f
  if c = $\top$ or f = $\bot$ or f = $\top$
    return f
  x := top variable in f // the root of the BDD
  if (x = top variable in c)  
    return (restrict(f$|_{\mbox{x}}$, c$|_{\mbox{x}}$) $\lor$ restrict(f$|_{\lnot\mbox{x}}$, c$|_{\mbox{x}}$))
  else // x does not appear in c
    return (x $\land$ restrict(f$|_{\mbox{x}}$ , c)) $\lor$ ($\lnot$x $\land$ restrict(f$|_{\lnot\mbox{x}}$ , c))
end
$\\$
\end{lstlisting}

In the context of symbolic model checking, existential quantification of a set of variable $X$ from a function $f$ can be done in CUDD by a call to \texttt{Cudd\_bddExistAbstract(f, c)}, where \texttt{f} is the BDD for $f$ and \texttt{c} is a function essentially depending on all the variables in $X$ (in MCMAS \texttt{c} is set to be the conjunction of all the variables in $X$).

In the SDD Package, existential quantification can only be done one variable at a time, by calling \texttt{sdd\_exists(x, f, manager)} which returns $\exists x f$. Our first solution was to iteratively call this function on every variable in $X$, but our first experiments revealed that this method was very slow. We therefore implemented the above algorithm in our model checker as an attempt to reduce computation time:

\begin{lstlisting}[mathescape]

SddNode* restrict_sdd(SddNode* f, SddNode* c, SddManager* manager) 
{
  if(sdd_node_is_false(c)) // not allowed
    return NULL; 
  if(sdd_node_is_true(c) || sdd_node_is_false(f) || sdd_node_is_true(f))
    return f;
  SddLiteral x = get_top_variable(f);
  SddLiteral y = get_top_variable(c);
  if (x == y) {
    SddNode* res1 = restrict_sdd(sdd_condition(x, f, manager), sdd_condition(x, c, manager), manager);
    SddNode* res2 = restrict_sdd(sdd_condition(-x, f, manager), sdd_condition(x, c, manager), manager);
    return sdd_disjoin(res1, res2, manager);
  } else {
    SddNode* res1 = restrict_sdd(sdd_condition(x, f, manager), c, manager);
    SddNode* res2 = restrict_sdd(sdd_condition(-x, f, manager), c, manager);
    return ite(sdd_manager_literal(x, manager), res1, res2, manager);
  }
}

\end{lstlisting}
In the code above, we have omitted the calls to \texttt{sdd\_ref()} and \texttt{sdd\_deref()} for clarity. Also, \texttt{ite} refers to an implementation of the ITE (If-Then-Else) operator on Boolean functions, namely $$\mbox{ITE}(f, g, h) = (f \land g) \lor (\lnot f \land h).$$

Another point worth discussing is the notion of ``top variable" for an SDD. In a BDD, this is unambiguously refering to the variable with which the root is labelled, and this choice of variable for the start of the algorithm is clearly induced by the conditioning algorithm on BDDs, which consists in removing the \textit{false} outcoming edge for each node labelled with the variable in question -- if this is the top variable, then this amounts to simply removing the right (or left) subtree of the BDD. 

In an SDD however, this is not so obvious: 

to do: talk about \texttt{sdd\_condition()} and the performance of our implementation.



\subsection{SDD-Specific Features}

\subsubsection{Vtrees and Variable Orders}

Recall that in the SDD Package, the use of the standard vtrees is only possible when a variable order is supplied. For convenience, we implemented the four standard MCMAS variable orders in our model checker, for use with one of the standard vtree, thereby making 16 different vtrees available. These orderings are implemented within the \texttt{get\_var\_order()} function, whose code is documented in appendix. 

As explained in the evaluation section of this report, these 16 vtrees quickly proved insufficient for our purposes, and being able to create our own ``non-standard" vtrees became crucial for the progress of the project.  
The SDD Package API is lacking some basic functions in that area, most probably because it is generally sufficient for users to rely on the standard vtrees, together with the dynamic minimisation feature. There are, for example, no ways of constructing a vtree bottom-up, as one would construct an SDD node. To create our own customised vtrees, we had the two following options:
\begin{itemize}
\item Generate a variable order to create one of the standard vtrees, and apply swaps, left- and right-rotations until the vtree obtained is the desired vtree
\item Generate a \textit{vtree format} text file and use \texttt{sdd\_vtree\_read()} to create a \texttt{Vtree} object. According to the SDD manual, this file format is normally used to save vtrees to file in order to re-use them in the future. The function \texttt{sdd\_vtree\_save\_to\_file()} is the way to generate the file from an existing vtree.

\end{itemize}
We chose the second option, simply because it seemed more convenient than computing the set of operations to apply to get from one vtree to another. The vtree format, without being particularly readable, has the advantage of being relatively simple in syntax and therefore easy to generate. 

The API of the SDD Package was clearly not designed to edit and manipulate vtrees practically, so as a replacement we created a new tree structure, the \texttt{vtree\_node}, defined thus: 
\begin{verbatim}
struct vtree_node 
{
  // the children
  vtree_node * left; 
  vtree_node * right;
  // more data for file generation
  int size;
  bool isleaf;
  int id;
};
\end{verbatim}

This very simple structure significantly facilitated the process of constructing vtrees. We could simply create a leaf \texttt{vtree\_node} for each variable needed, and construct the vtree bottom-up. Observe that the variables themselves are not stored; this is because we organise the nodes so that each variable is \textit{one more} than the ID of the leaf which contains it (IDs start from 0 in the file, whereas variables are numbered from 1 in the SDD manager).

 The function \texttt{vtree\_node\_get\_file\_content()} was written to recursively return the string representing the \texttt{vtree\_node} in the file format required by \texttt{sdd\_vtree\_read()}. The code for the function, as well as an example, can be found in the appendix (To do). 

In practical applications, the number of variables is not known in advance but we still need a way of generating the vtree dynamically. To this aim, we wrote the \texttt{create\_vtree()} function which is called in the program as soon as the number of variables needed is known, but \textit{before} the SDD manager is created, so that the vtree produced can be used during its initialisation -- note that this is only possible since our vtree is not based on actual variables but on a temporary structure which does not involve the manager. 

To use \texttt{create\_vtree()}, we need to pass it the number of variables that the vtree should consist of, as well as the desired \textit{vtree type}. This is an integer corresponding to one of the various vtrees which we implemented: vtree types 1-4 correspond to the standard vtrees defined in \ref{std_vtrees}, whereas types 4-8 are ``new" vtrees which we found to have more potential in the context of model checking. We leave out the details for now and use the evaluation section to describe the process leading to their discovery. 

\subsubsection{Dynamic Vtree Search in Multi-Agent Systems}

In \ref{sdd_dao_algo}, we described the dynamic minimisation algorithm implemented in the SDD Package. This algorithm is focused on a complete restructuring of the manager's vtree in order to minimise the size of its SDD nodes.
Early experiments with this algorithm \textit{sometimes }revealed large overheads due to the difficulty of searching the huge space of vtrees. We propose a new approach to vtree search adapted to multi-agent systems in those cases where the original approach is not efficient. In this section we focus on the implementation of this new approach, while a detailed description, and a practical comparison of both techniques is detailed in the evaluation section (\ref{comparison_dynamic_approaches}).

The new approach to vtree search is analogous to some dynamic variable reordering techniques for BDDs, which involve defining \textit{groups} of variables, set to remain contiguous during the reordering process. Using those techniques, reordering can then be applied within each group, and groups themselves can be reordered. 

We propose a comparable algorithm for vtree search where the variables for individual agents are grouped together into sub-vtrees, and the minimisation function is called on each of these separately. Note that in the SDD Package, the default implementation of the minimisation function can be modified using \texttt{sdd\_manager\_set\_minimize\_function()},  indicating which function should be used instead.
The code for our replacement implementation is as follows: 

\begin{lstlisting} [mathescape]
Vtree* vtree_group_minimize(Vtree* vtree, SddManager* manager) {
  // start from the root   
  Vtree* current = vtree; 
  // repeat until a leaf is reached
  while (!sdd_vtree_is_leaf(sdd_vtree_right(current))) {
    // minimise the left subtree using original algorithm
    sdd_vtree_minimize(sdd_vtree_left(current), manager);
    // and go down one step to the right
    current = sdd_vtree_right(current);	
  }
  sdd_vtree_minimize(current);
  return vtree;
} 
\end{lstlisting}

This algorithm is appropriate for vtrees of a particular shape, which will constitute the focus our study in most of the evaluation section of this report. Without revealing too much information at this point, we simply say
that these vtrees are built so that variables are separated into smaller groups, each group forming a left "branch" in the vtree. 
Figure~\ref{fig:first_pseudo_right_linear} illustrate this with an example, showing the effect of \texttt{vtree\_group\_minimize()} on a particular vtree.

\begin{figure}
\centering
\includegraphics[scale=0.5]{pseudorightlinearfirstexample.png}
\caption{If called on this vtree, the \texttt{vtree\_group\_minimize()} function will attempt to minimise the subtrees rooted at nodes 26, 21, 28, and 29, while keeping these nodes, as well as their ancestors (30, 31, 32), unchanged.}
\label{fig:first_pseudo_right_linear}
\end{figure}

\subsection{Software Engineering Issues and Challenges}

Here we explain some of the challenges faced during the implementation of the model checker. 

\subsubsection{Garbage Collection}

As described in \ref{gc}, the SDD Package has a automatic garbage collection feature based on reference counts, but it leaves the referencing and dereferencing of nodes to the user. 

If not done properly, node referencing can lead to issues such as unwanted garbage collection, dead nodes kept in memory, or dead node dereferencing, all of which being very undesirable (but for different reasons).

This forced the constant track-keeping of node reference counts and required more debugging time. Nonetheless, it is important to mention that the manual referencing and dereferencing of nodes is probably more efficient in the long run than if the SDD manager had to do it in the background, which would require more thinking on its part.

\subsubsection{Comparing SDDs and BDDs}

The goal of this implementation was to build an SDD-based model checker which produced the same results as MCMAS in all circumstances. We needed to make sure that the Boolean functions representating our sets of states and transition relations were the same at each step of our model checking algorithm. Unfortunately, there is no convenient and precise way of programmatically comparing an SDD with a BDD. 

The only \textit{exact} method available is to look at the Boolean function corresponding to each data structure, and compare these. However, due to their syntactic differences, it had to be done using a SAT checker (we used \cite{minisat}), which is a good solution for small functions but becomes impractical very quickly when the number of variable exceeds about 20 (which happens in all non-trivial cases).  

An useful alternative is to construct the SDD with respect to a right-linear vtree and the BDD with the equivalent variable ordering (see \ref{bdds_are_sdds} for details), to ensure that the resulting structures are comparable. We then have two options:
\begin{itemize}

\item Compare the graphical representation of each data structure, provided by both APIs via a DOT file \cite{dot}. An image is often enough to tell if two representations are not equivalent, but in the case where they are, it can be a very long process to manually verify it.
\item Compare the size (i.e. the number of nodes) of each structure. Again, most of the time two non-equivalent representations have very different sizes, but representations with comparable sizes are not necessarily equivalent (note that due to the way SDD and BDD nodes are counted, the size of a BDD is not exactly the same as the size of the equivalent SDD so this method is not 100\% conclusive either).

\end{itemize}
 This second option is nonetheless the only available solution in the case of very large SDDs and BDDs, which explains the difficulty in debugging larger examples.

Reflecting back on this issue, it would have been much less time-consuming to write a small logic equivalence tool ensuring that the Boolean functions obtained from two equivalent data structures was not only equivalent, but also \textit{syntactically equal}, to reduce the problem to a simple comparison of strings. We did not anticipate enough the amount of work that would be required when debugging the model checker.

\subsubsection{Correctness}

 Due to the reasons mentioned above, we had to use a few simple tricks to make sure that our code was indeed an implementation of the model checking algorithm. 

The first technique we used was that of comparing the \textit{output} of our model checker with that of MCMAS, on the set of examples that we had available (see \ref{ispl_examples} for a detailed description of these examples). Once we were certain that these were identical (so no obvious bugs existed), we did a more thorough search to ensure that there was nothing wrong with the code. 

Although our set of examples was relatively diverse, it certainly did not cover the whole range of expressions, data types, etc. available in ISPL, nor did it use the full set of CTLK connectives that ISPL supports.
Hence, our second approach to verification was to try and "invent" new formulae to check, using different combinations of connectives from those remaining. Furthermore, 

\section{Evaluation (?)} 

\subsection{Introduction}

\subsubsection{Evaluation Strategy}

The ultimate objective of this project was either to establish, or to refute SDDs (in their current state) as serious competitors to BDDs in model checking. The obvious plan for quantitative analysis was to do a side-by-side comparison of our model checker and MCMAS on a series of examples, and conclude. However, in order to provide a fair comparison we had to ensure that both structures were performing ``at the best of their abilities". 

There are suggested methods for the use of BDDs in model checking, in particular concerning initial variable orders~\cite{bdd_heuristics}. For the specific case of MCMAS we observe that the standard order number 2 (see \ref{std_orderings}) \textit{often} yields the best results.
On the other hand, SDDs had not yet been explored in the context of model checking, and consequently no heuristics existed. 

Through this evaluation we planned to remediate this by investigating various vtree constructions and comparing the resulting model checker with MCMAS. 

To start with, we decided to focus on the issue of \textit{static} vtree generation, for the situation where the vtree has to be determined \textit{before} SDDs are constructed, as opposed to continuously modified as the construction process happens -- this is called \textit{dynamic} minimisation. 

Although it may seem like wasted effort, the importance of static vtree generation is non-negligible, for two reasons: firstly, it helps us understand what aspects of the vtree have the greatest impact on the final SDD; secondly, even though dynamic minimisation is generally the most efficient technique, it can also be a very time-consuming process which some applications might find less practical. 

After this first investigation, we planned to look at the performance of SDDs when enabling the dynamic minimisation feature of the SDD Package, and compare it to BDD dynamic variable reordering as implemented in MCMAS.

We hoped that both of these analyses (static and dynamic) would provide enough information to know whether or not SDDs (as implemented in the SDD Package) are suitable for model checking multi-agent systems.

	\subsubsection{Example Models}
	\label{ispl_examples}
\begin{itemize}
\item Bit transmission problem 
\item Dining Cryptographers
\item Prisoners' dilemma 
\item NSPK - 
http://en.wikipedia.org/wiki/Needham-Schroeder\_protocol
\end{itemize}

also: choice of examples, why parametrised, describe the state spaces of each

\subsubsection{Experimental Protocol} 

Throughout this chapter, we report a large number of tables containing experimental results. In this section we attempt to describe as accurately as possible the framework in which these experiments took place, as well as the format in which we present the results. 

All benchmarks were run on ...

Most results are split into two tables: time and memory; time measurements are in seconds, memory usage in megabytes.

In each table we indicate in bold the situations where SDDs outperform BDDs.

in most cases we set parameters to be the most appropriate

\subsection{Static Comparisons with Standard Vtrees}

We started our investigation by experimenting with the standard vtrees, namely right-linear, left-linear, balanced, and vertical. Recall that this first investigation was focused exclusively on static vtree generation, so we disabled \textit{both }the dynamic SDD minimisation feature in our model checker, and the CUDD dynamic reordering feature in MCMAS. As automatic garbage collection was disabled in 

We started with right-linear vtrees. 

\subsubsection{Right-Linear Vtrees} 

Recall that BDDs are structurally identical to SDDs build using a right-linear vtree, and the same variable ordering (\ref{bdds_are_sdds}). Experimenting with this particular type of vtrees was therefore of significant importance, as it would allow us to compare both model checkers in the case where they are handling data structures of the exact same size.

Tables \ref{table:rightlineartime} and \ref{table:rightlinearmemory} correspond, respectively, to the time and memory comparisons of our model checker with MCMAS, when SDDs are built using right-linear vtrees inducing the standard orderings 1, 2, 3 and 4. 

\begin{table}
\centering
\begin{tabular}{|*{9}{c|}}
\hline
 & \multicolumn{2}{|c|}{Ordering 1} & \multicolumn{2}{|c|}{Ordering 2} & \multicolumn{2}{|c|}{Ordering 3} & \multicolumn{2}{|c|}{Ordering 4}\\
\cline{2-9}
& BDDs & SDDs & BDDs & SDDs & BDDs & SDDs & BDDs & SDDs\\ \hline
cryptos7 & 2.06 & 12.35 & 2.64 &  6.39 & 3.33 & 12.77 & 3.83 & 15.186 \\ \hline
cryptos8 & 11.85 & 56.02 & 17.24 & 30.42 & 20.41 & 58.70 & 23.85 & 69.54 \\ \hline
cryptos9 & 853.17 & \textbf{221.16} & 98.95 & 145.313 & 617.32 & \textbf{266.84} & 911.70 & \textbf{305.17} \\ \hline
pris9 & 50.14 & - & 11.47 & 252.21 & 12.52 & 369.83 & 17.31 & - \\ \hline
pris10 & 236.15 & - & 35.11 & - & 42.12 & -& 52.13 & -\\ \hline
nspk1 & 0.06 & 0.79 & 0.04 & 0.59 & 0.09 & 1.21 & 0.06 & 0.71 \\ \hline
nspk2 & 11.25 & 154.46 & 2.71 & 76.32 & 7.36 & 120.42 & 4.55& 84.15 \\ \hline
nspk4 & 1.29 & 27.29 & 0.32 & 9.79 & 1.75 & 23.74 & 0.95 & 14.68 \\ \hline
nspk5 & - & - & 158.62 & - & - & - & - & - \\ \hline
nspk6 & - & - & - &- & - & - & - & - \\ \hline
\end{tabular}
\\
\caption{Computation Time: SDDs were built with right-linear vtrees, and standard orderings}
\label{table:rightlineartime}
\end{table}

\begin{table}
\centering
\begin{tabular}{|*{9}{c|}}
\hline
\multirow{2}{*} & \multicolumn{2}{|c|}{Ordering 1} & \multicolumn{2}{|c|}{Ordering 2} & \multicolumn{2}{|c|}{Ordering 3} & \multicolumn{2}{|c|}{Ordering 4}\\
\cline{2-9}
& BDDs & SDDs & BDDs & SDDs & BDDs & SDDs & BDDs & SDDs\\ \hline
cryptos7 & 198 & 782 & 160 & 306 & 248 & 656 & 272 & 791 \\ \hline
cryptos8 & 758 & 3120 & 647 & 1308 & 1024 & 2630 & 1120 & 3166  \\ \hline
cryptos9 & 3001 & 11556 & 2850 & 5566 & 4246 & 10134 & 4690 & 11927 \\ \hline
pris9 & 594 & - & 313 & 7658 & 3502 & 12382 & 4349 & -\\ \hline
pris10 & 1955 & - & 847 & - & 968 & - & 1218 & -\\ \hline
nspk1 & 16 & 34 & 13 & 24 & 25 & 50 & 17 & 34 \\ \hline
nspk2 & 324 & 2065 & 135 & 1012 & 493 & 1520 & 275 & 1104 \\ \hline
nspk4 & 92 & 624 & 45 & 223  & 188 & 693 & 81 & 344 \\ \hline
nspk5 & - & - & 3841 & - & - & - & - & -\\ \hline
nspk6 & -&- &- &-  &- &- &- &- \\ \hline
\end{tabular}
\\
\caption{Memory Usage: SDDs were built with right-linear vtrees, and standard orderings}
\label{table:rightlinearmemory}
\end{table}

During this first phase of the investigation, it became quickly apparent that SDDs were \textit{not }better than BDDs in this situation, outperforming them in only three cases, and being slower in most cases (sometimes considerably, up to two orders of magnitude). The important difference in memory usage is explained by the garbage collector being off in the SDD Package. 

We observed one thing in particular. Despite the SDD/BDD ratio being relatively consistent within each class of examples, it varies a lot more across the different classes. Going back to the characteristics of these classes, we realise that this ratio increases with the size of the model  
\\\\\\


To conclude this first set of experiments, we make the following remark: in this situation the model checkers are dealing with the \textit{exact} same structures, so these numbers \textit{should }be the same for BDDs and SDDs. But they aren't, for the reasons explained above and in \ref{comparison_with_cudd}. However, \textit{should the SDD Package ever become as efficient as CUDD, any improvement on the SDD numbers in Tables~\ref{table:rightlineartime} and \ref{table:rightlinearmemory} would be an improvement on BDDs.} This is hypothetical, but it motivated the rest of our experiments, and in particular, the search for a better vtree.
\\\\
\textit{Note on BDD data: from now on, all the BDD data found in the comparison tables corresponds to computations using standard ordering number 2, the most efficient for BDDs in the vast majority of cases (\ref{std_orderings}).
}

\subsubsection{Other Standard Vtrees}

We moved on from right-linear vtrees, hoping that one of the other standard vtrees would lead to more efficient computations. This was not the case. 



\begin{table}
\begin{tabular}{|*{10}{c|}}
\hline
\multirow{2}{*}{} & \multirow{2}{*}{BDDs} & \multicolumn{4}{c|}{Left-Linear} & \multicolumn{4}{c|}{Vertical}\\ \cline{3-10}
& & O. 1 & O. 2 & O. 3 & O. 4  & O. 1 & O. 2 & O. 3 & O. 4 \\ \hline
btp & 0.02& 44.3 & 8.36 & 6.42 & 6.56 & 0.61 & 0.13 & 0.09 & 0.10 \\ \hline
cryptos4 & 0.02 & - & - & - & - & - & 21.72 & 57.67 & 47.15 \\ \hline
pris3 & 0.01 & - & - & - & - & 1267.91 & 3.14 & 4.56 & 4.34 \\ \hline
pris5 & 0.04 &- &- &- &- & - &-  &- & -\\ \hline
nspk1 & 0.03 & - & - & & & & & & \\ \hline
\end{tabular}
\caption{Time Comparisons with left-linear vtrees.}
\label{table:otherstandardvtrees1time}
\end{table}
~
\begin{table}
\centering
\begin{tabular}{|*{10}{c|}}
\hline
\multirow{2}{*}{} & \multirow{2}{*}{BDDs} & \multicolumn{4}{c|}{Left-Linear} & \multicolumn{4}{c|}{Vertical}\\ \cline{3-10}
& & O. 1 & O. 2 & O. 3 & O. 4  & O. 1 & O. 2 & O. 3 & O. 4 \\ \hline
btp & 9 & 559 & 176 & 122 & 121 & 15 & 4 & 4 & 4 \\ \hline
cryptos4 & 11 & - & - & -& - & -  & 267 & 522 & 407 \\ \hline
pris3 & 9 & - & - & - & - & 7272 & 48 & 73 & 55 \\ \hline
pris5 & 13 & - &- &- &- &- &- &-&- \\ \hline
nspk1 & 12 & - &- & & & & & & \\ \hline
\end{tabular}
\caption{Memory with left-linear vtrees.}
\label{table:otherstandardvtrees1memory}
\end{table}

Left-linear and vertical vtrees proved \textit{extremely} inefficient. As shown in Tables~\ref{table:otherstandardvtrees1time} and \ref{table:otherstandardvtrees1memory}, even on the small examples chosen they were significantly slower than BDDs. Furthermore, although the variable ordering chosen for the vtrees seems to have a big impact on the computation (much bigger in fact than on BDDs), even for the better ones there is no competition.  

Before pushing the analysis further, we look at balanced vtrees; these are slightly more efficient, but still lead to much higher computations times than BDDs, or even right-linear vtrees (to do: check that). In Tables~\ref{table:otherstandardvtrees2memory} and \ref{table:otherstandardvtrees2memory}.

\begin{table}
\centering
\begin{tabular}{|*{9}{c|}}
\hline
 & \multirow{2}{*}{BDDs} & \multicolumn{4}{c|}{Balanced} \\ \cline{3-6}
 & & O. 1& O. 2 & O.3 & O.4 \\ \hline
cryptos5 & & 0.92 & & & \\ \hline
cryptos6 & & 5.78 & & & \\ \hline
cryptos7 & & & & & \\ \hline
pris5 & & & & & \\ \hline
pris6 & & & & & \\ \hline
pris9 & & & & & \\ \hline
nspk1 & & & & & \\ \hline
nspk2 & & & & & \\ \hline
nspk4 & & & & & \\ \hline
\end{tabular}
\caption{Balanced Vtrees: Computation Time}
\label{table:otherstandardvtrees2time}
\end{table}
\begin{table}
\centering
\begin{tabular}{|*{9}{c|}}
\hline
 & \multirow{2}{*}{BDDs} & \multicolumn{4}{c|}{Balanced} \\ \cline{3-6}
 & & O. 1& O. 2 & O.3 & O.4 \\ \hline
cryptos5 & & 29 & & & \\ \hline
cryptos6 & & 136 & & & \\ \hline
cryptos7 & & & & & \\ \hline
pris5 & & & & & \\ \hline
pris6 & & & & & \\ \hline
pris9 & & & & & \\ \hline
nspk1 & & & & & \\ \hline
nspk2 & & & & & \\ \hline
nspk4 & & & & & \\ \hline
\end{tabular}
\caption{Balanced Vtrees: Memory Usage}
\label{table:otherstandardvtrees2memory}
\end{table}


\subsubsection{Towards a Better Vtree: Observations}

Upon experimenting with all the 16 standard vtrees, it was clear that if SDDs were ever to be used in model checking, there would need to be a way of generating better vtrees. 

\myparagraph{What Is a Good Vtree?}

Our first investigation was an attempt at finding out how, for such small models, computation times could be so high. SDD nodes could not possibly be very large, so the overhead had to be elsewhere. We realised that the main characteristic that left-linear, vertical, and (to a lesser extend) balanced vtrees have in common is the size (i.e. the number of descendents) of the left branches of their nodes. 
This is a \textit{very }important property, for the following reason. 

Recall the construction of an SDD for function $f$ with respect to a vtree $v$. It involves creating an $X$-partition of $f$, where $X$ is the set of variables in $v^l$. If $X$ contains a lot of the variables in $f$, the \textit{subs} of $f$ will be large. But in SDDs, \textit{decisions} are based on \textit{sentences}, i.e. on subs, so if these are large, decisions will be slower to make. But SDD operations such as \texttt{apply} or \texttt{condition} rely on decisions, and these operations are the basis for the model checking algorithm. Hence the overhead.

This is of course very much simplified, and if it were entirely true there would be absolutely no point in trying to replace BDDs (whose \textit{subs} are as light as possible -- they are just one literal!) with SDDs. But the general idea is there, and the ultimate objective of vtree optimisation is to construct subs in such a way that: \begin{itemize}
\item The number of decisions to make is less than it would be in a BDD (in order to do this we simply put more than one variable in the left branches of the vtree),
\item Decisions remain quick enough (this is the tricky part).
\end{itemize}

\myparagraph{Analysis of an Example}

In order to get a better idea of the type of vtree which would make the computation more efficient, we ran the model checker on a set of examples using the dynamic minimisation feature, and we looked at the final vtree that the search algorithm produced (i.e. the manager's vtree immediately before the process terminated). The result was unambiguous: in \textit{all} cases, independently of the initial vtree, the final vtree had the same shape, a sort of right-linear vtree where the left branches contain a set of variables rather than single variable. For this vague description to make more sense, we illustrate it here with an example, shown in Figure~\ref{fig:large_vtree_after_reordering}. 
We make a few remarks about the shape of this vtree (without examiniming the variable order yet):
\begin{sidewaysfigure}
\includegraphics[scale=0.3]{largevtreeafterdao.png}
\caption{The manager's vtree at the end of a run of our model checker on crypto6, with an initial vtree of type balanced.}
\label{fig:large_vtree_after_reordering}
\end{sidewaysfigure}


\begin{itemize}
\item The vtree in Figure~\ref{fig:large_vtree_after_reordering} is \textit{typical}, and an absolutely fair representative of the vtrees obtained after dynamic minimisation (whatever the example, and whatever the initial vtree).
\item Recall that the dynamic minimisation algorithm is ``symmetric" in the sense that it does \textit{not} force the vtree in one particular direction. It considers 24 vtrees (of which 12 are induced from the l-vtree by applying one of the usual vtree operations, and 12 arise from the r-vtree) and picks the best one. The fact that we get this vtree shape in all cases is evidence that it is in fact the \textit{ideal} configuration. 
\item It tends to happen (and it is definitely the case in Figure~\ref{fig:large_vtree_after_reordering}) that the first few left-branches are slightly smaller than the ones below. A reasonable explanation for this is that the first few left-branches correspond to the subs in the first few levels of the corresponding SDD (starting from the root), 
\item 
\item 
\end{itemize}





we run the dynamic minimisation feature, what we find: 
\begin{itemize}
\item vtrees are pseudo right-linear
\item how are variables arranged
\item look at how vtree changes during the process
\end{itemize}
if we come up with new static heuristics, they need to make sense and we should be able to generate them automatically from the model 


\subsection{Static Comparisons with Alternative Vtrees}

\subsubsection{A First Attempt}

inspired from MCMAS orderings we split the variables by agent

\subsubsection{An Upper Bound on Subtree Size}

important to explain why this improves computation times
\subsubsection{Various Vtree Characteristics and Their Impact}

\begin{itemize}
\item changing the way variables are allocated to subtrees
\item changing variable order \textit{within} subtrees and subtree type
\item changing subtree order, and possibly adapting compute\_reach algorithm
\item 

\end{itemize}

We aim to find an initial vtree leading to faster computations. We notice that whatever the initial vtree, dynamic reduction algorithms always result in a particular type of vtree, which we call \textit{pseudo-right-linear}. 
All our experiments are with pseudo-right-linear vtrees.
\begin{itemize}
\item vtree experiment 1 (option 5): one balanced subtree per agent. 

This does \textit{not} work well.
\item vtree experiment 2 (option 6): We set a maximum size for agent subtrees. An upper bound of $\log_2(n^2)$ (where $n$ is the number of vars) has proved relatively efficient. If an agent has more variables then we create more subtrees for it. Variables of a subtree come from the same agent. Subtrees are balanced, and we experiment with different orderings for them. We observe that the best results are obtained when two principles are followed: action variables are close to each other in the subtree (i.e they alone form a balanced vtree of size \texttt{action\_count}) and states variables are 
paired with their primed counterpart. (TODO: confirm this!)

\item vtree experiment 3 (option 7): Each subtree contains one variable for each agent, as well as its corresponding primed variable. Action variables for each agent form their own subtree. We order state subtrees 'largest to smallest' and put action subtrees at the bottom. This seems to be the best ordering but TODO need to try: intercaler actions/states.

\item vtree experiment 4 (option 8): Think of something! 

\end{itemize}

\subsection{Using Dynamic Reordering and Minimisation}

\subsubsection{Initial Observations}

Raw data with all examples and stuff 

\subsubsection{Experiments}

\begin{itemize}
\item 
\item 
\end{itemize}

\subsubsection{Grouping Variables}
new vtree search method 
\subsection{Summary}

review of best techniques and heuristics

\subsection{Qualitative Evaluation}

Do SDDs compare with BDDs?

\section{Conclusion and future work}

\subsection{Review}

\subsection{Future work}

Missing features will be counterexample/witness generation, and checking for deadlock or model overflow. Also being able to check ATL formulas.
At some point I would like to have a go at implementing an ADD equivalent for SDDs.
-> Call \texttt{sdd\_apply()} on a reduced vtree


\begin{thebibliography}{9}

\bibitem{original_model_checking} E. M. Clarke, E. A. Emerson: \textit{Design and synthesis of synchronization skeletons using branching time temporal logic}, 1981.

\bibitem{birth_of_model_checking} E. M. Clarke: \textit{The Birth Of Model Checking}, 2008.

\bibitem{symbolic_model_checking} J. P. Queille, J. Sifakis: \textit{Specification and Verification of Concurrent Systems in CESAR}, 1981.

\bibitem{symbolic} K. L. McMillan, \textit{Symbolic Model Checking: an Approach to the State Explosion Problem}, PhD Thesis, 1992. 


\bibitem{sdd_1}
A. Darwiche:\emph{ SDD: A New Canonical Representation of Propositional Knowledge Bases}, 2011.

\bibitem{sdd_2}
A. Darwiche, A. Choi, Y. Xue: \emph{Basing Decisions on Sentences}, 2012.

\bibitem{sdd_3}
A. Darwiche, A. Choi: \textit{Dynamic Minimization of Sentential Decision Diagrams}, 2013.

\bibitem{compilation_map}
  A. Darwiche, P. Marquis:
  \emph{A Knowledge Compilation Map}, 2002.

\bibitem{trees_of_bdds}
S. Subbarayan, L. Bordeaux, Y. Hamadi:\textit{
Knowledge Compilation Properties of Tree-of-BDDs}, 2007.

\bibitem{bdd_trees}
K. L. McMillan:\textit{ Hierarchical representations of discrete functions, with application to model checking, 1994.
}

\bibitem{logic_in_computer_science}
 M. Huth, M. Ryan, 
\emph{Logic in Computer Science},
Cambridge University Press, 2004.

\bibitem{knuth} D. E. Knuth, \textit{The Art of Computer Programming, Volume
4, Fascicle 2: Generating All Tuples and Permutations}, Addison-Wesley Professional, 2005.

\bibitem{add}  R. I. Bahar, E. A. Frohm, C. M. Gaona, G. D. Hachtel, E. Macii, A. Pardo, F. Somenzi:\textit{
Algebraic decision diagrams and their applications}, 1993.

\bibitem{bdd_heuristics} M. Rice, S. Kulhari:\textit{ A Survey of Static Variable Ordering Heuristics for Efficient BDD/MDD Construction}, 2008.

\bibitem{interleaving} H. Fujii, G. Ootomo, C. Hori: \textit{Interleaving Based Variable Ordering Methods for Ordered Binary Decision Diagrams}, 1993.

\bibitem{bryant} R. E. Bryant: \textit{Graph-Based Algorithms
for Boolean Function Manipulation}, 1986.

\bibitem{ctl*}  	E. A. Emerson, J. Y. Halpern:\textit{
``Sometimes" and ``not never" revisited: on branching versus linear time temporal logic}, 1986.
	
\bibitem{ctlk}R. Fagin, J. Y. Halpern, Y. Moses, M. Y. Vardi: \textit{Reasoning about Knowledge}, 1995.

\bibitem{unified_framework} O. Coudert, J-C. Madre: \textit{A Unified Framework For the Formal Verification of Sequential Circuits}, 1990.

\bibitem{MCMAS_website} MCMAS webpage, http://vas.doc.ic.ac.uk/software/mcmas/

\bibitem{CUDD_website} CUDD Package webpage, http://vlsi.colorado.edu/~fabio/CUDD/

\bibitem{SDD_website} SDD Package webpage, http://reasoning.cs.ucla.edu/sdd/

\bibitem{minisat} David A. Wheeler's MiniSAT solver,  http://www.dwheeler.com/essays/minisat-user-guide.html

\bibitem{dot}Graphviz DOT, http://www.graphviz.org/Documentation.php

\bibitem{notes_303} A. Lomuscio, Software Engineering: Software Verifications, lecture notes (as taught in 2012-13)

\bibitem{notes_499} I. Hodkinson, Modal and Temporal logic, lecture notes (as taught in 2013-14)

\end{thebibliography}

\appendix

\section{The Bit Transmission Problem}

\subsection{The Problem}

\subsection{The Model}

\subsection{ISPL Specification}

\section{Implementation Details}

Code:
\begin{itemize}
\item the SDD params structure
\item the compute reach function
\item the getvarorder and createvtree functions
\end{itemize}

\end{document}
